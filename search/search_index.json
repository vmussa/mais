{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trabalhando pela universaliza\u00e7\u00e3o do uso de dados no Brasil Base dos Dados Mais A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados no Brasil. Acreditamos que a dist\u00e2ncia entre uma pessoa e uma an\u00e1lise deveria ser apenas uma boa ideia. Para realizar essa vis\u00e3o, n\u00f3s constru\u00edmos a Base dos Dados Mais (BD+): um reposit\u00f3rio integrado de dados. Essa ferramenta traz acesso, rapidez, escala, facilidade, economia, curadoria, e transpar\u00eancia ao cen\u00e1rio de dados no Brasil. Todos os nossos dados ficam organizados e dispon\u00edveis na nuvem dentro da ferramenta da Google chamada BigQuery . Uma simples consulta de SQL \u00e9 o suficiente para cruzamento de bases que voc\u00ea desejar. Sem precisar procurar, baixar, tratar, comprar um servidor e subir clusters na nuvem. Por que o BigQuery? Acesso : \u00c9 poss\u00edvel deixar os dados p\u00fablicos, i.e., qualquer pessoa com uma conta no Google Cloud pode fazer uma query na base, quando quiser. Rapidez : Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala : O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Facilidade : Voc\u00ea pode cruzar tabelas tratadas e atualizadas num s\u00f3 lugar. Economia : O custo \u00e9 praticamente zero para usu\u00e1rios - 1 TB gratuito por m\u00eas para usar como quiser . Depois disso, s\u00e3o cobrados somente 5 d\u00f3lares por TB de dados que sua query percorrer. Clique para acessar o projeto no BigQuery Quick Start Acesse os dados direto pelo BigQuery \ud83d\udd0d Acesse os dados pelo seu computador (CLI/API) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Como citar o projeto O projeto est\u00e1 licenciado sob a Licen\u00e7a Hipocr\u00e1tica . Sempre que usar os dados cite a fonte como: Portugu\u00eas: Carabetta, Jo\u00e3o; Dahis, Ricardo; Israel, Fred; Scovino, Fernanda (2020) Base dos Dados: Reposit\u00f3rio de Dados Abertos em https://basedosdados.org. Ingl\u00eas: Carabetta, Jo\u00e3o; Dahis, Ricardo; Israel, Fred; Scovino, Fernanda (2020) Data Basis: Open Data Repository at https://basedosdados.org. Idiomas Documenta\u00e7\u00e3o est\u00e1 em portugu\u00eas (quando poss\u00edvel), c\u00f3digo e configura\u00e7\u00f5es est\u00e3o em ingl\u00eas.","title":"Introdu\u00e7\u00e3o"},{"location":"#base-dos-dados-mais","text":"A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados no Brasil. Acreditamos que a dist\u00e2ncia entre uma pessoa e uma an\u00e1lise deveria ser apenas uma boa ideia. Para realizar essa vis\u00e3o, n\u00f3s constru\u00edmos a Base dos Dados Mais (BD+): um reposit\u00f3rio integrado de dados. Essa ferramenta traz acesso, rapidez, escala, facilidade, economia, curadoria, e transpar\u00eancia ao cen\u00e1rio de dados no Brasil. Todos os nossos dados ficam organizados e dispon\u00edveis na nuvem dentro da ferramenta da Google chamada BigQuery . Uma simples consulta de SQL \u00e9 o suficiente para cruzamento de bases que voc\u00ea desejar. Sem precisar procurar, baixar, tratar, comprar um servidor e subir clusters na nuvem.","title":"Base dos Dados Mais"},{"location":"#por-que-o-bigquery","text":"Acesso : \u00c9 poss\u00edvel deixar os dados p\u00fablicos, i.e., qualquer pessoa com uma conta no Google Cloud pode fazer uma query na base, quando quiser. Rapidez : Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala : O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Facilidade : Voc\u00ea pode cruzar tabelas tratadas e atualizadas num s\u00f3 lugar. Economia : O custo \u00e9 praticamente zero para usu\u00e1rios - 1 TB gratuito por m\u00eas para usar como quiser . Depois disso, s\u00e3o cobrados somente 5 d\u00f3lares por TB de dados que sua query percorrer. Clique para acessar o projeto no BigQuery","title":"Por que o BigQuery?"},{"location":"#quick-start","text":"Acesse os dados direto pelo BigQuery \ud83d\udd0d Acesse os dados pelo seu computador (CLI/API) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb","title":"Quick Start"},{"location":"#como-citar-o-projeto","text":"O projeto est\u00e1 licenciado sob a Licen\u00e7a Hipocr\u00e1tica . Sempre que usar os dados cite a fonte como: Portugu\u00eas: Carabetta, Jo\u00e3o; Dahis, Ricardo; Israel, Fred; Scovino, Fernanda (2020) Base dos Dados: Reposit\u00f3rio de Dados Abertos em https://basedosdados.org. Ingl\u00eas: Carabetta, Jo\u00e3o; Dahis, Ricardo; Israel, Fred; Scovino, Fernanda (2020) Data Basis: Open Data Repository at https://basedosdados.org.","title":"Como citar o projeto"},{"location":"#idiomas","text":"Documenta\u00e7\u00e3o est\u00e1 em portugu\u00eas (quando poss\u00edvel), c\u00f3digo e configura\u00e7\u00f5es est\u00e3o em ingl\u00eas.","title":"Idiomas"},{"location":"access_data_bq/","text":"Como usar via BigQuery Ao clicar no bot\u00e3o voc\u00ea ser\u00e1 redirecionado para logar na sua conta ou criar uma antes de acessar o projeto. Clique para acessar o projeto no BigQuery Na sua tela dever\u00e1 aparecer o projeto fixado no menu lateral esquerdo, como na imagem abaixo. Criando uma conta no BigQuery \u00c9 preciso, basicamente, ter uma conta Google para acessar o BigQuery. O site deve solicitar que voc\u00ea crie um projeto qualquer no seu BigQuery antes de acessar os nossos dados - n\u00e3o se preocupe, n\u00e3o \u00e9 pago! O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais sobre o Sandbox aqui . Acessando o projeto Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o, datasets (conjuntos de dados) e tables (tabelas), nos quais: Todas as tables est\u00e3o organizadas em datasets Cada table pertence a um \u00fanico dataset Caso n\u00e3o apare\u00e7am as tabelas nos datasets do projeto na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina. Explorando os dados Exemplo: Qual a evolu\u00e7\u00e3o do PIB per capita de todos os munic\u00edpios? \ud83d\udcc8 O BigQuery utiliza SQL como linguagem nativa. Leia mais sobre a sintaxe utilizada aqui . Rode a query abaixo no Query Editor/Editor de consultas e obtenha o cruzamento das tabelas de popula\u00e7\u00e3o e PIB do IBGE com o resultado anual desde 1991. SELECT pib . id_municipio , pop . ano , pib . PIB / pop . populacao * 1000 AS pib_per_capita FROM ` basedosdados . br_ibge_pib . municipios ` AS pib JOIN ` basedosdados . br_ibge_populacao . municipios ` AS pop ON pib . id_municipio = pop . id_municipio AND pib . ano = pop . ano Dica Clicando no bot\u00e3o \ud83d\udd0d Consultar tabela/Query View , o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em Query Editor/Editor de consultas - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios. Entenda os dados O BigQuery possui j\u00e1 um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais na se\u00e7\u00e3o de Nomenclatura . Metadados Clicando num dataset ou table voc\u00ea j\u00e1 consegue ver toda a estrutura e descri\u00e7\u00e3o das colunas, e pode acessar tamb\u00e9m os detalhes de tratamento e publica\u00e7\u00e3o, para entender melhor os dados.","title":"BigQuery"},{"location":"access_data_bq/#como-usar-via-bigquery","text":"Ao clicar no bot\u00e3o voc\u00ea ser\u00e1 redirecionado para logar na sua conta ou criar uma antes de acessar o projeto. Clique para acessar o projeto no BigQuery Na sua tela dever\u00e1 aparecer o projeto fixado no menu lateral esquerdo, como na imagem abaixo.","title":"Como usar via BigQuery"},{"location":"access_data_bq/#criando-uma-conta-no-bigquery","text":"\u00c9 preciso, basicamente, ter uma conta Google para acessar o BigQuery. O site deve solicitar que voc\u00ea crie um projeto qualquer no seu BigQuery antes de acessar os nossos dados - n\u00e3o se preocupe, n\u00e3o \u00e9 pago! O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais sobre o Sandbox aqui .","title":"Criando uma conta no BigQuery"},{"location":"access_data_bq/#acessando-o-projeto","text":"Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o, datasets (conjuntos de dados) e tables (tabelas), nos quais: Todas as tables est\u00e3o organizadas em datasets Cada table pertence a um \u00fanico dataset Caso n\u00e3o apare\u00e7am as tabelas nos datasets do projeto na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina.","title":"Acessando o projeto"},{"location":"access_data_bq/#explorando-os-dados","text":"","title":"Explorando os dados"},{"location":"access_data_bq/#exemplo-qual-a-evolucao-do-pib-per-capita-de-todos-os-municipios","text":"O BigQuery utiliza SQL como linguagem nativa. Leia mais sobre a sintaxe utilizada aqui . Rode a query abaixo no Query Editor/Editor de consultas e obtenha o cruzamento das tabelas de popula\u00e7\u00e3o e PIB do IBGE com o resultado anual desde 1991. SELECT pib . id_municipio , pop . ano , pib . PIB / pop . populacao * 1000 AS pib_per_capita FROM ` basedosdados . br_ibge_pib . municipios ` AS pib JOIN ` basedosdados . br_ibge_populacao . municipios ` AS pop ON pib . id_municipio = pop . id_municipio AND pib . ano = pop . ano Dica Clicando no bot\u00e3o \ud83d\udd0d Consultar tabela/Query View , o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em Query Editor/Editor de consultas - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios.","title":"Exemplo: Qual a evolu\u00e7\u00e3o do PIB per capita de todos os munic\u00edpios? \ud83d\udcc8"},{"location":"access_data_bq/#entenda-os-dados","text":"O BigQuery possui j\u00e1 um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais na se\u00e7\u00e3o de Nomenclatura .","title":"Entenda os dados"},{"location":"access_data_bq/#metadados","text":"Clicando num dataset ou table voc\u00ea j\u00e1 consegue ver toda a estrutura e descri\u00e7\u00e3o das colunas, e pode acessar tamb\u00e9m os detalhes de tratamento e publica\u00e7\u00e3o, para entender melhor os dados.","title":"Metadados"},{"location":"access_data_local/","text":"Como acessar os dados localmente $ pip install basedosdados Em apenas 3 passos voc\u00ea consegue obter dados estruturados para baixar e analisar: Instalar a aplica\u00e7\u00e3o Criar um projeto no Google Cloud Realizar sua query para explorar os dados Instalando a aplica\u00e7\u00e3o CLI pip install basedosdados Python pip install basedosdados R install.packages ( \"basedosdados\" ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)! Criando um projeto no Google Cloud Caso j\u00e1 tenha um projeto pr\u00f3prio, v\u00e1 direto para a pr\u00f3xima etapa! Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso reposit\u00f3rio p\u00fablico. Basta seguir o passo-a-passo: Acesse o link: https://console.cloud.google.com/projectselector2/home/dashboard Aceite o Termo de Servi\u00e7os do Google Cloud Clique em Create Project/Criar Projeto Escolha um nome bacana para o seu projeto :) Clique em Create/Criar Veja que seu projeto tem um Nome e um Project ID - este segundo \u00e9 a informa\u00e7\u00e3o que voc\u00ea ir\u00e1 utilizar em <YOUR_PROJECT_ID> para fazer queries no nosso reposit\u00f3rio p\u00fablico. Fazendo queries Utilize todo o poder do BigQuery onde quiser. Para obter, filtrar ou cruzar bases basta escrever a query e carregar em sua linguagem favorita. Abaixo voc\u00ea pode seguir um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros em todos os anos dispon\u00edveis . CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar uma tabela inteira no pandas -- por padr\u00e3o, `query_project_id` # \u00e9 o basedosdados, voc\u00ea pode usar esse par\u00e2metro para escolher outro projeto df = bd . read_table ( dataset_id = 'br_ibge_populacao' , table_id = 'municipios' , billing_project_id =< YOUR_PROJECT_ID > , limit = 100 ) Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) R if ( ! require ( \"basedosdados\" )) install.packages ( \"basedosdados\" ) library ( \"basedosdados\" ) set_billing_id ( \"<YOUR_PROJECT_ID>\" ) query <- \"SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" # Voc\u00ea pode fazer o download no seu computador dir <- tempdir () data <- download ( query , file.path ( dir , \"pib_per_capita.csv\" )) # Ou carregar o resultado da query no seu ambiente de an\u00e1lise data <- read_sql ( query ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Local"},{"location":"access_data_local/#como-acessar-os-dados-localmente","text":"$ pip install basedosdados Em apenas 3 passos voc\u00ea consegue obter dados estruturados para baixar e analisar: Instalar a aplica\u00e7\u00e3o Criar um projeto no Google Cloud Realizar sua query para explorar os dados","title":"Como acessar os dados localmente"},{"location":"access_data_local/#instalando-a-aplicacao","text":"CLI pip install basedosdados Python pip install basedosdados R install.packages ( \"basedosdados\" ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Instalando a aplica\u00e7\u00e3o"},{"location":"access_data_local/#criando-um-projeto-no-google-cloud","text":"Caso j\u00e1 tenha um projeto pr\u00f3prio, v\u00e1 direto para a pr\u00f3xima etapa! Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso reposit\u00f3rio p\u00fablico. Basta seguir o passo-a-passo: Acesse o link: https://console.cloud.google.com/projectselector2/home/dashboard Aceite o Termo de Servi\u00e7os do Google Cloud Clique em Create Project/Criar Projeto Escolha um nome bacana para o seu projeto :) Clique em Create/Criar Veja que seu projeto tem um Nome e um Project ID - este segundo \u00e9 a informa\u00e7\u00e3o que voc\u00ea ir\u00e1 utilizar em <YOUR_PROJECT_ID> para fazer queries no nosso reposit\u00f3rio p\u00fablico.","title":"Criando um projeto no Google Cloud"},{"location":"access_data_local/#fazendo-queries","text":"Utilize todo o poder do BigQuery onde quiser. Para obter, filtrar ou cruzar bases basta escrever a query e carregar em sua linguagem favorita. Abaixo voc\u00ea pode seguir um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros em todos os anos dispon\u00edveis . CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar uma tabela inteira no pandas -- por padr\u00e3o, `query_project_id` # \u00e9 o basedosdados, voc\u00ea pode usar esse par\u00e2metro para escolher outro projeto df = bd . read_table ( dataset_id = 'br_ibge_populacao' , table_id = 'municipios' , billing_project_id =< YOUR_PROJECT_ID > , limit = 100 ) Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) R if ( ! require ( \"basedosdados\" )) install.packages ( \"basedosdados\" ) library ( \"basedosdados\" ) set_billing_id ( \"<YOUR_PROJECT_ID>\" ) query <- \"SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" # Voc\u00ea pode fazer o download no seu computador dir <- tempdir () data <- download ( query , file.path ( dir , \"pib_per_capita.csv\" )) # Ou carregar o resultado da query no seu ambiente de an\u00e1lise data <- read_sql ( query ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Fazendo queries"},{"location":"analysis/","text":"Se divertindo com a BD+ Todo esse esfor\u00e7o tem uma recompensa: criamos um banco de dados integrado com diversas bases pronto para an\u00e1lises e visualiza\u00e7\u00f5es. Nessa se\u00e7\u00e3o descrevemos coisas que podem ser \u00fateis aos usu\u00e1rios de diversos p\u00fablicos. Tem ideias de an\u00e1lises, gr\u00e1ficos, ou reportagens para escrever em cima do nosso reposit\u00f3rio? Fique a vontade pra come\u00e7ar! S\u00f3 n\u00e3o esque\u00e7a de citar o projeto ;) Como cruzar tabelas no BD+ Podemos cruzar tabelas na BD+ fazendo joins em cima de chaves externas ( foreign keys ). Essas s\u00e3o colunas que servem para identificar unicamente entidades no reposit\u00f3rio. Crit\u00e9rios para uma vari\u00e1vel ser uma chave externa: Nome da vari\u00e1vel \u00e9 \u00fanico entre todos as bases do reposit\u00f3rio BD+; Identifica unicamente a entidade e n\u00e3o tem valores nulos na tabela correspondente de diret\u00f3rio. Chaves geogr\u00e1ficas Setor censit\u00e1rio: id_setor_censitario Munic\u00edpio: id_municipio (padr\u00e3o), id_municipio_6 , id_municipio_tse , id_municipio_rf , id_municipio_bcb \u00c1rea M\u00ednima Compar\u00e1vel: id_AMC Regi\u00e3o imediata: id_regiao_imediata Regi\u00e3o intermedi\u00e1ria: id_regiao_intermediaria Microrregi\u00e3o: id_microrregiao Mesorregi\u00e3o: id_mesorregiao Unidade da federa\u00e7\u00e3o (UF): sigla_uf (padr\u00e3o), id_uf , uf Regi\u00e3o: regiao Chaves temporais ano , semestre , mes , semana , dia , hora Chaves de pessoas f\u00edsicas cpf , pis , nis Chaves de pessoas jur\u00eddicas Empresa: cnpj Escola: id_escola Chaves em pol\u00edtica Candidato(a): id_candidato_bd Partido: sigla_partido , partido","title":"In\u00edcio"},{"location":"analysis/#se-divertindo-com-a-bd","text":"Todo esse esfor\u00e7o tem uma recompensa: criamos um banco de dados integrado com diversas bases pronto para an\u00e1lises e visualiza\u00e7\u00f5es. Nessa se\u00e7\u00e3o descrevemos coisas que podem ser \u00fateis aos usu\u00e1rios de diversos p\u00fablicos. Tem ideias de an\u00e1lises, gr\u00e1ficos, ou reportagens para escrever em cima do nosso reposit\u00f3rio? Fique a vontade pra come\u00e7ar! S\u00f3 n\u00e3o esque\u00e7a de citar o projeto ;)","title":"Se divertindo com a BD+"},{"location":"analysis/#como-cruzar-tabelas-no-bd","text":"Podemos cruzar tabelas na BD+ fazendo joins em cima de chaves externas ( foreign keys ). Essas s\u00e3o colunas que servem para identificar unicamente entidades no reposit\u00f3rio. Crit\u00e9rios para uma vari\u00e1vel ser uma chave externa: Nome da vari\u00e1vel \u00e9 \u00fanico entre todos as bases do reposit\u00f3rio BD+; Identifica unicamente a entidade e n\u00e3o tem valores nulos na tabela correspondente de diret\u00f3rio.","title":"Como cruzar tabelas no BD+"},{"location":"analysis/#chaves-geograficas","text":"Setor censit\u00e1rio: id_setor_censitario Munic\u00edpio: id_municipio (padr\u00e3o), id_municipio_6 , id_municipio_tse , id_municipio_rf , id_municipio_bcb \u00c1rea M\u00ednima Compar\u00e1vel: id_AMC Regi\u00e3o imediata: id_regiao_imediata Regi\u00e3o intermedi\u00e1ria: id_regiao_intermediaria Microrregi\u00e3o: id_microrregiao Mesorregi\u00e3o: id_mesorregiao Unidade da federa\u00e7\u00e3o (UF): sigla_uf (padr\u00e3o), id_uf , uf Regi\u00e3o: regiao","title":"Chaves geogr\u00e1ficas"},{"location":"analysis/#chaves-temporais","text":"ano , semestre , mes , semana , dia , hora","title":"Chaves temporais"},{"location":"analysis/#chaves-de-pessoas-fisicas","text":"cpf , pis , nis","title":"Chaves de pessoas f\u00edsicas"},{"location":"analysis/#chaves-de-pessoas-juridicas","text":"Empresa: cnpj Escola: id_escola","title":"Chaves de pessoas jur\u00eddicas"},{"location":"analysis/#chaves-em-politica","text":"Candidato(a): id_candidato_bd Partido: sigla_partido , partido","title":"Chaves em pol\u00edtica"},{"location":"colab_checks/","text":"Colaborando com testes na BD+ Para manter a qualidade dos bases de dados presentes na BD+, n\u00f3s contamos com um conjunto de checagens autom\u00e1ticas que s\u00e3o realizadas durante a inser\u00e7\u00e3o e atualiza\u00e7\u00e3o de cada base. Essas checagens s\u00e3o necess\u00e1rias, mas n\u00e3o suficientes para garantir a qualidade dos dados. Elas realizam consultas basicas, como se a tabela existe ou se tem colunas totalmente nulas. Voc\u00ea pode colaborar com a BD aumentando a cobertura dos testes, diminuindo assim o trabalho de revis\u00e3o dos dados. Para isso basta criar consultas que testem a qualidade dos dados em SQL, como as seguintes: Verificar se colunas com propor\u00e7\u00e3o possuem valores entre 0 e 100 Verificar se colunas com datas seguem o padr\u00e3o YYYY-MM-DD HH:MM:SS Qual o procedimento? Incluir testes de dados deve seguir o fluxo de trabalho: Informe seu interesse Escreva sua consulta Submeta sua consulta Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :) 1. Informe seu interesse Converse conosco no bate-papo da infra ou reuni\u00f5es \u00e0s 19h da segunda-feira, ambos no Discord. Caso n\u00e3o tenha uma sugest\u00e3o de melhoria podemos procurar alguma consulta que ainda n\u00e3o foi escrita. 2. Escreva sua consulta Fa\u00e7a um fork do reposit\u00f3rio da Base dos Dados+ . Em seguida adicione novas consultas e suas respectivas fun\u00e7\u00f5es de execu\u00e7\u00e3o nos arquivos checks.yaml e test_data.py . As consultas s\u00e3o escritas em um arquivo YAML com Jinja e SQL, da forma: test_select_all_works : name : Check if select query in {{ table_id }} works query : | SELECT NOT EXISTS ( SELECT * FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}` ) AS failure E executadas como testes do pacote pytest : def test_select_all_works ( configs ): result = fetch_data ( \"test_select_all_works\" , configs ) assert result . failure . values == False N\u00e3o se assuste caso n\u00e3o conhe\u00e7a algo da sintaxe acima, podemos lhe ajudar durante o processo. Note que os valores entre chaves s\u00e3o vari\u00e1veis contidas em arquivos table_config.yaml , que cont\u00e9m metadados das tabelas. Logo a escrita de consulta \u00e9 limitada pelos metadados existentes. Recomendamos consultar estes arquivos no diret\u00f3rio das bases . 3. Submeta sua consulta Por fim realize um pull request para o reposit\u00f3rio principal para que seja realizada uma revis\u00e3o da consulta.","title":"Testes de Dados"},{"location":"colab_checks/#colaborando-com-testes-na-bd","text":"Para manter a qualidade dos bases de dados presentes na BD+, n\u00f3s contamos com um conjunto de checagens autom\u00e1ticas que s\u00e3o realizadas durante a inser\u00e7\u00e3o e atualiza\u00e7\u00e3o de cada base. Essas checagens s\u00e3o necess\u00e1rias, mas n\u00e3o suficientes para garantir a qualidade dos dados. Elas realizam consultas basicas, como se a tabela existe ou se tem colunas totalmente nulas. Voc\u00ea pode colaborar com a BD aumentando a cobertura dos testes, diminuindo assim o trabalho de revis\u00e3o dos dados. Para isso basta criar consultas que testem a qualidade dos dados em SQL, como as seguintes: Verificar se colunas com propor\u00e7\u00e3o possuem valores entre 0 e 100 Verificar se colunas com datas seguem o padr\u00e3o YYYY-MM-DD HH:MM:SS","title":"Colaborando com testes na BD+"},{"location":"colab_checks/#qual-o-procedimento","text":"Incluir testes de dados deve seguir o fluxo de trabalho: Informe seu interesse Escreva sua consulta Submeta sua consulta Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)","title":"Qual o procedimento?"},{"location":"colab_checks/#1-informe-seu-interesse","text":"Converse conosco no bate-papo da infra ou reuni\u00f5es \u00e0s 19h da segunda-feira, ambos no Discord. Caso n\u00e3o tenha uma sugest\u00e3o de melhoria podemos procurar alguma consulta que ainda n\u00e3o foi escrita.","title":"1. Informe seu interesse"},{"location":"colab_checks/#2-escreva-sua-consulta","text":"Fa\u00e7a um fork do reposit\u00f3rio da Base dos Dados+ . Em seguida adicione novas consultas e suas respectivas fun\u00e7\u00f5es de execu\u00e7\u00e3o nos arquivos checks.yaml e test_data.py . As consultas s\u00e3o escritas em um arquivo YAML com Jinja e SQL, da forma: test_select_all_works : name : Check if select query in {{ table_id }} works query : | SELECT NOT EXISTS ( SELECT * FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}` ) AS failure E executadas como testes do pacote pytest : def test_select_all_works ( configs ): result = fetch_data ( \"test_select_all_works\" , configs ) assert result . failure . values == False N\u00e3o se assuste caso n\u00e3o conhe\u00e7a algo da sintaxe acima, podemos lhe ajudar durante o processo. Note que os valores entre chaves s\u00e3o vari\u00e1veis contidas em arquivos table_config.yaml , que cont\u00e9m metadados das tabelas. Logo a escrita de consulta \u00e9 limitada pelos metadados existentes. Recomendamos consultar estes arquivos no diret\u00f3rio das bases .","title":"2. Escreva sua consulta"},{"location":"colab_checks/#3-submeta-sua-consulta","text":"Por fim realize um pull request para o reposit\u00f3rio principal para que seja realizada uma revis\u00e3o da consulta.","title":"3. Submeta sua consulta"},{"location":"colab_data/","text":"Dados na BD+ Nosso objetivo na Base dos Dados Mais (BD+) \u00e9 criar um reposit\u00f3rio de dados universal e de alta qualidade : Universal : Isso significa, em princ\u00edpio, incluir todas as bases de dados do Brasil e do mundo. Pouco ambicioso n\u00e9 :-). Alta qualidade : Ter alta qualidade significa manter todos nossos dados estruturados, limpos, bem documentados, consistentes entre si e atualizados. Isso j\u00e1 \u00e9 um desafio em projetos pequenos, e \u00e9 ainda mais em escala . Por isso, desenvolvemos e seguimos uma s\u00e9rie de padr\u00f5es e procedimentos de controle de qualidade, e criamos uma infraestrutura para facilitar a subida de dados por qualquer pessoa ou institui\u00e7\u00e3o (do nosso time de dados ou colaboradores externos). Colaborando com Dados Quer subir dados na BD+ e nos ajudar a construir esse reposit\u00f3rio? Maravilha! Vamos seguir com os passos abaixo. Ao longo da explica\u00e7\u00e3o, vamos sempre seguir um exemplo j\u00e1 pronto com dados da RAIS . Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :) Qual o procedimento? Informe seu interesse para a gente . Baixar nossa pasta template para dados . Preencher as tabelas de arquitetura . Escrever c\u00f3digo de captura e limpeza de dados . Organizar arquivos auxiliares, se necess\u00e1rio . Criar tabela dicion\u00e1rio, se necess\u00e1rio . Subir arquiteturas, dados e arquivos auxiliares no Google Cloud . Enviar tudo para revis\u00e3o . 1. Informe seu interesse para a gente Mantemos metadados de bases que ainda n\u00e3o est\u00e3o na BD+ em nossa tabela de prioridade de bases . Para subir uma base de seu interesse na BD+, adicione os seus metadados de acordo com as colunas da tabela. Inclua uma linha por tabela do conjunto de dados. Preencha os campos cuidadosamente . Al\u00e9m dos dados gerais sobre a tabela, adicione as classifica\u00e7\u00f5es de Facilidade e Import\u00e2ncia e o campo de Prioridade ser\u00e1 gerado automaticamente. Por fim, crie um issue no Github com o template \"Novos dados\" Caso sua base j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github na coluna Pessoa respons\u00e1vel . Fique \u00e0 vontade para conversar com a gente e tirar d\u00favidas direto no nosso servidor no Discord . 2. Baixar nossa pasta template para dados Baixe aqui a nossa pasta template para colabora\u00e7\u00e3o de dados, e renomeie ela para <dataset_id> . Ela facilita todos os passos daqui pra frente. Sua estrutura \u00e9 bem simples: /<dataset_id> /code Cont\u00e9m todos os scripts (c\u00f3digos) necess\u00e1rios \u00e0 captura e limpeza dos dados (discutido no passo 4 ). Nessa configura\u00e7\u00e3o, toda a estrutura de c\u00f3digo ser\u00e1 com atalhos relativos \u00e0 pasta ra\u00edz, usando as demais pastas criadas. /input Cont\u00e9m todos os arquivos com dados originais, exatamente como baixados da fonte prim\u00e1ria. Esses arquivos n\u00e3o devem jamais ser modificados. /output Cont\u00e9m arquivos finais, j\u00e1 no formato pronto para subir na BD+. /tmp Cont\u00e9m quaisquer arquivos tempor\u00e1rios criados pelo c\u00f3digo em /code no processo de limpeza e tratamento. /extra /architecture Cont\u00e9m as tabelas de arquitetura (discutido no passo 3 ). /auxiliary_files Cont\u00e9m quaisquer arquivos auxiliares aos dados (discutido no passo 5 ). dicionario.csv Tabela dicion\u00e1rio mapeando chaves a valores para todas as tabelas da base (discutido no passo 6 ). 3. Preencher as tabelas de arquitetura Definir e preencher as tabelas de arquitetura s\u00e3o partes fundamentais da colabora\u00e7\u00e3o de dados. \u00c9 aqui que (1) definimos como estar\u00e3o estruturadas as tabelas em produ\u00e7\u00e3o, (2) discutimos nomea\u00e7\u00e3o e ordenamento de colunas, (3) preenchemos os metadados de colunas, e (4) compatibilizamos dados entre anos quando h\u00e1 inconsist\u00eancias. Cada base tem 1 ou mais tabelas de dados. Cada tabela de dados tem sua tabela de arquitetura. Tabelas de arquitetura podem ser preenchidas no Google Drive ou localmente (Excel, editor de texto). Perguntas que uma arquitetura deve responder: Quais ser\u00e3o as tabelas finais na base? Essas n\u00e3o precisam ser exatamente o que veio nos dados brutos. Qual \u00e9 n\u00edvel da observa\u00e7\u00e3o de cada tabela? O \"n\u00edvel da observa\u00e7\u00e3o\" \u00e9 a menor unidade a que se refere cada linha na tabela (ex: municipio-ano, candidato, estabelecimento-cnae). Ser\u00e1 gerada alguma tabela derivada com agrega\u00e7\u00f5es em cima dos dados originais? No exemplo da RAIS, aqui est\u00e3o as tabelas de arquitetura j\u00e1 preenchidas. Sempre seguindo nosso manual de estilo , n\u00f3s renomeamos, definimos o tipo, preenchemos descri\u00e7\u00f5es, indicamos se h\u00e1 dicion\u00e1rio ou diret\u00f3rio, preenchemos campos (e.g. cobertura temporal e unidade de medida) e fizemos a compatibiliza\u00e7\u00e3o entre anos para todas as vari\u00e1veis (colunas). A compatibiliza\u00e7\u00e3o entre anos, caso necess\u00e1ria, \u00e9 feita criando novas colunas \u00e0 direita chamadas nome_original_YYYY , em ordem temporal descendente (2021, 2020, 2019, ...). Nessas colunas inclu\u00edmos todas as vari\u00e1veis de cada ano. Para as que forem eventualmente exclu\u00eddas da vers\u00e3o em produ\u00e7\u00e3o, deixamos seu nome como (deletado) e n\u00e3o preenchemos nenhum metadado. Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados ou nossa comunidade para validar tudo. \u00c9 importante ter certeza que est\u00e1 fazendo sentido antes de come\u00e7ar a escrever c\u00f3digo. 4. Escrever c\u00f3digo de captura e limpeza de dados Depois de validadas as tabelas de arquitetura podemos escrever o c\u00f3digo de captura e limpeza dos dados. Exigimos que tudo esteja escrito em Python , R , ou Stata . Podem ser c\u00f3digo padr\u00e3o ou cadernos (Colab, Jupyter, Rmarkdown, etc). No exemplo da RAIS, temos todo o c\u00f3digo escrito em Stata para consulta aqui . Captura Esse script baixa automaticamente todos os dados originais e os salva em /input . Esses dados podem estar dispon\u00edveis em portais ou links FTP, podem ser raspados de sites, entre outros. Limpeza Esse script transforma os dados originais salvos em /input nos dados limpos prontos para serem subidos na BD+ salvos em /output , tudo baseado nas tabelas de arquitetura. Cada tabela limpa para produ\u00e7\u00e3o pode ser salva como um arquivo .csv \u00fanico ou, caso seja muito grande (e.g. acima de 100-200 mb), ser particionada no formato Hive em v\u00e1rios sub-arquivos .csv . Nossa recomenda\u00e7\u00e3o \u00e9 particionar tabelas por ano , mes , sigla_uf , ou no m\u00e1ximo por id_municipio . Para a tabela microdados_vinculos da RAIS n\u00f3s particionamos por ano e sigla_uf , com a estrutura de pastas sendo /microdados_vinculos/ano=YYYY/sigla_uf=XX . N\u00f3s j\u00e1 criamos fun\u00e7\u00f5es \u00fateis para limpeza nas nossas APIs de Python e R. Por exemplo, com o pacote basedosdados voc\u00ea pode ler tabelas muito grandes, particionar tabelas automaticamente, gerar certas vari\u00e1veis comuns (e.g. sigla_uf a partir de id_uf ), etc. Tudo nesse passo deve seguir nosso manual de estilo e as melhores pr\u00e1ticas de programa\u00e7\u00e3o . 5. Se necess\u00e1rio, organizar arquivos auxiliares \u00c9 comum bases de dados serem distribu\u00eddas com arquivos auxiliares. Esses podem incluir notas t\u00e9cnicas, descri\u00e7\u00f5es de coleta e amostragem, etc. Para ajudar usu\u00e1rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em /extra/auxiliary_files . Fique \u00e0 vontade para estruturar sub-pastas como quiser l\u00e1 dentro. O que importa \u00e9 que fique claro o que s\u00e3o esses arquivos. 6. Se necess\u00e1rio, criar tabela dicion\u00e1rio \u00c9 tamb\u00e9m comum que bases de dados grandes sejam estruturadas de forma a precisar de um dicion\u00e1rio. Para poupar espa\u00e7o, vari\u00e1veis STRING s\u00e3o convertidas em vari\u00e1veis num\u00e9ricas com cada chave mapeando um valor \u00fanico. Muitas vezes, especialmente com bases antigas, h\u00e1 m\u00faltiplos dicion\u00e1rios em formatos Excel ou outros. Na Base dos Dados n\u00f3s unificamos tudo em um \u00fanico arquivo em formato .csv . Descrevemos nossas v\u00e1rias diretrizes para dicion\u00e1rios no nosso manual de estilo . Por exemplo: Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas). Para cada tabela, coluna e cobertura temporal, cada chave mapeia unicamente um valor. Chaves n\u00e3o podem ter valores nulos. Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais. Chaves n\u00e3o possuem zeros \u00e0 esquerda. Exemplo: 01 deveria ser 1 . Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc. No exemplo da RAIS n\u00f3s criamos um dicion\u00e1rio completo aqui . 7. Subir arquiteturas, dados e arquivos auxiliares no Google Cloud Tudo pronto! Agora s\u00f3 falta subir as coisas para o Google Cloud e depois enviar para revis\u00e3o. Desenvolvemos um cliente basedosdados (dispon\u00edvel para linha de comando e Python por enquanto) para facilitar esse processo e indicar configura\u00e7\u00f5es b\u00e1sicas que devem ser preenchidas sobre os dados. Configure seu projeto no Google Cloud e um bucket no Google Storage Os dados v\u00e3o passar ao todo por 3 lugares no Google Cloud: Storage: local onde ser\u00e3o armazenados o arquivos (arquiteturas, dados, arquivos auxiliares). BigQuery: banco de dados do Google, dividido em 2 projetos/tipos de tabela: Staging: banco para teste e tratamento final do conjunto de dados Produ\u00e7\u00e3o: banco oficial de publica\u00e7\u00e3o dos dados ( basedosdados ! ou o seu mesmo caso queira reproduzir o ambiente) Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. Seguindo o passo-a-passo: Acesse o link e aceite o Termo de Servi\u00e7os do Google Cloud. Clique em Create Project/Criar Projeto - escolha um nome bacana para o seu projeto, ele ter\u00e1 tamb\u00e9m um Project ID que ser\u00e1 utilizado para configura\u00e7\u00e3o local. Depois de criado o projeto, v\u00e1 at\u00e9 a funcionalidade de Storage e crie uma pasta, seu bucket , para voc\u00ea subir os dados. Configure o CLI localmente, clone nosso reposit\u00f3rio e abra uma nova branch No seu terminal: Instale nosso cliente: pip install basedosdados . Rode basedosdados config init e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Clone um fork do nosso reposit\u00f3rio localmente. D\u00ea um cd para a pasta local do reposit\u00f3rio e abra uma nova branch com git checkout -b [BRANCH_ID] . Todas as adi\u00e7\u00f5es e modifica\u00e7\u00f5es ser\u00e3o feitas nessa branch . Suba e configure uma tabela no seu bucket Aqui s\u00e3o dois passos: primeiro publicamos uma base e depois publicamos tabelas. Publique uma base. Rode o comando basedosdados dataset create [DATASET_ID] --raw_path '/[DATASET_ID]/input' --auxiliary_files_path '/[DATASET_ID]/extra/auxiliary_files' . Preencha os arquivos de configura\u00e7\u00e3o da base: README.md : informa\u00e7\u00f5es b\u00e1sicas da base de dados aparecendo no Github. dataset_config.yaml : informa\u00e7\u00f5es espec\u00edficas da base de dados. Rode o comando basedosdados dataset update [DATASET_ID] para atualizar a base com as configura\u00e7\u00f5es preenchidas. Publique uma tabela (ou v\u00e1rias!) dentro da base Rode o comando basedosdados table create [DATASET_ID] [TABLE_ID] --data_path '/[DATASET_ID]/output/[TABLE_ID]' --architecture_path '/[DATASET_ID]/extra/architecture/[TABLE_ID]' . Preencha os arquivos de configura\u00e7\u00e3o da tabela: /[TABLE_ID]/table_config.yaml : informa\u00e7\u00f5es espec\u00edficas da tabela. /[TABLE_ID]/publish.sql : aqui voc\u00ea pode fazer tratamentos finais na tabela staging em SQL para publica\u00e7\u00e3o. Exemplo: modificar a query para dar um JOIN em outra tabela da BD+ e selecionar vari\u00e1veis. Rode o comando basedosdados table publish [DATASET_ID] [TABLE_ID] para publicar a tabela em produ\u00e7\u00e3o. Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo. \u00c9 sempre bom abrir o console do BigQuery e rodar algumas queries para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom\u00e1ticos para facilitar esse processo no futuro. 8. Enviar tudo para revis\u00e3o Ufa, \u00e9 isso! Agora s\u00f3 resta enviar tudo para revis\u00e3o no reposit\u00f3rio da Base dos Dados. Crie os commits necess\u00e1rios e rode um git push origin [BRANCH_ID] . Depois \u00e9 s\u00f3 abrir um pull request (PR) no nosso reposit\u00f3rio. Temos passos manuais e autom\u00e1ticos de revis\u00e3o para dados e metadados. Pessoas do time da Base dos Dados entrar\u00e3o em contato com voc\u00ea para pedir mudan\u00e7as ou tirar d\u00favidas. Quando tudo estiver redondo n\u00f3s fazemos um merge e os dados s\u00e3o publicados na nossa plataforma.","title":"Dados"},{"location":"colab_data/#dados-na-bd","text":"Nosso objetivo na Base dos Dados Mais (BD+) \u00e9 criar um reposit\u00f3rio de dados universal e de alta qualidade : Universal : Isso significa, em princ\u00edpio, incluir todas as bases de dados do Brasil e do mundo. Pouco ambicioso n\u00e9 :-). Alta qualidade : Ter alta qualidade significa manter todos nossos dados estruturados, limpos, bem documentados, consistentes entre si e atualizados. Isso j\u00e1 \u00e9 um desafio em projetos pequenos, e \u00e9 ainda mais em escala . Por isso, desenvolvemos e seguimos uma s\u00e9rie de padr\u00f5es e procedimentos de controle de qualidade, e criamos uma infraestrutura para facilitar a subida de dados por qualquer pessoa ou institui\u00e7\u00e3o (do nosso time de dados ou colaboradores externos).","title":"Dados na BD+"},{"location":"colab_data/#colaborando-com-dados","text":"Quer subir dados na BD+ e nos ajudar a construir esse reposit\u00f3rio? Maravilha! Vamos seguir com os passos abaixo. Ao longo da explica\u00e7\u00e3o, vamos sempre seguir um exemplo j\u00e1 pronto com dados da RAIS . Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)","title":"Colaborando com Dados"},{"location":"colab_data/#qual-o-procedimento","text":"Informe seu interesse para a gente . Baixar nossa pasta template para dados . Preencher as tabelas de arquitetura . Escrever c\u00f3digo de captura e limpeza de dados . Organizar arquivos auxiliares, se necess\u00e1rio . Criar tabela dicion\u00e1rio, se necess\u00e1rio . Subir arquiteturas, dados e arquivos auxiliares no Google Cloud . Enviar tudo para revis\u00e3o .","title":"Qual o procedimento?"},{"location":"colab_data/#1-informe-seu-interesse-para-a-gente","text":"Mantemos metadados de bases que ainda n\u00e3o est\u00e3o na BD+ em nossa tabela de prioridade de bases . Para subir uma base de seu interesse na BD+, adicione os seus metadados de acordo com as colunas da tabela. Inclua uma linha por tabela do conjunto de dados. Preencha os campos cuidadosamente . Al\u00e9m dos dados gerais sobre a tabela, adicione as classifica\u00e7\u00f5es de Facilidade e Import\u00e2ncia e o campo de Prioridade ser\u00e1 gerado automaticamente. Por fim, crie um issue no Github com o template \"Novos dados\" Caso sua base j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github na coluna Pessoa respons\u00e1vel . Fique \u00e0 vontade para conversar com a gente e tirar d\u00favidas direto no nosso servidor no Discord .","title":"1. Informe seu interesse para a gente"},{"location":"colab_data/#2-baixar-nossa-pasta-template-para-dados","text":"Baixe aqui a nossa pasta template para colabora\u00e7\u00e3o de dados, e renomeie ela para <dataset_id> . Ela facilita todos os passos daqui pra frente. Sua estrutura \u00e9 bem simples: /<dataset_id> /code Cont\u00e9m todos os scripts (c\u00f3digos) necess\u00e1rios \u00e0 captura e limpeza dos dados (discutido no passo 4 ). Nessa configura\u00e7\u00e3o, toda a estrutura de c\u00f3digo ser\u00e1 com atalhos relativos \u00e0 pasta ra\u00edz, usando as demais pastas criadas. /input Cont\u00e9m todos os arquivos com dados originais, exatamente como baixados da fonte prim\u00e1ria. Esses arquivos n\u00e3o devem jamais ser modificados. /output Cont\u00e9m arquivos finais, j\u00e1 no formato pronto para subir na BD+. /tmp Cont\u00e9m quaisquer arquivos tempor\u00e1rios criados pelo c\u00f3digo em /code no processo de limpeza e tratamento. /extra /architecture Cont\u00e9m as tabelas de arquitetura (discutido no passo 3 ). /auxiliary_files Cont\u00e9m quaisquer arquivos auxiliares aos dados (discutido no passo 5 ). dicionario.csv Tabela dicion\u00e1rio mapeando chaves a valores para todas as tabelas da base (discutido no passo 6 ).","title":"2. Baixar nossa pasta template para dados"},{"location":"colab_data/#3-preencher-as-tabelas-de-arquitetura","text":"Definir e preencher as tabelas de arquitetura s\u00e3o partes fundamentais da colabora\u00e7\u00e3o de dados. \u00c9 aqui que (1) definimos como estar\u00e3o estruturadas as tabelas em produ\u00e7\u00e3o, (2) discutimos nomea\u00e7\u00e3o e ordenamento de colunas, (3) preenchemos os metadados de colunas, e (4) compatibilizamos dados entre anos quando h\u00e1 inconsist\u00eancias. Cada base tem 1 ou mais tabelas de dados. Cada tabela de dados tem sua tabela de arquitetura. Tabelas de arquitetura podem ser preenchidas no Google Drive ou localmente (Excel, editor de texto). Perguntas que uma arquitetura deve responder: Quais ser\u00e3o as tabelas finais na base? Essas n\u00e3o precisam ser exatamente o que veio nos dados brutos. Qual \u00e9 n\u00edvel da observa\u00e7\u00e3o de cada tabela? O \"n\u00edvel da observa\u00e7\u00e3o\" \u00e9 a menor unidade a que se refere cada linha na tabela (ex: municipio-ano, candidato, estabelecimento-cnae). Ser\u00e1 gerada alguma tabela derivada com agrega\u00e7\u00f5es em cima dos dados originais? No exemplo da RAIS, aqui est\u00e3o as tabelas de arquitetura j\u00e1 preenchidas. Sempre seguindo nosso manual de estilo , n\u00f3s renomeamos, definimos o tipo, preenchemos descri\u00e7\u00f5es, indicamos se h\u00e1 dicion\u00e1rio ou diret\u00f3rio, preenchemos campos (e.g. cobertura temporal e unidade de medida) e fizemos a compatibiliza\u00e7\u00e3o entre anos para todas as vari\u00e1veis (colunas). A compatibiliza\u00e7\u00e3o entre anos, caso necess\u00e1ria, \u00e9 feita criando novas colunas \u00e0 direita chamadas nome_original_YYYY , em ordem temporal descendente (2021, 2020, 2019, ...). Nessas colunas inclu\u00edmos todas as vari\u00e1veis de cada ano. Para as que forem eventualmente exclu\u00eddas da vers\u00e3o em produ\u00e7\u00e3o, deixamos seu nome como (deletado) e n\u00e3o preenchemos nenhum metadado. Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados ou nossa comunidade para validar tudo. \u00c9 importante ter certeza que est\u00e1 fazendo sentido antes de come\u00e7ar a escrever c\u00f3digo.","title":"3. Preencher as tabelas de arquitetura"},{"location":"colab_data/#4-escrever-codigo-de-captura-e-limpeza-de-dados","text":"Depois de validadas as tabelas de arquitetura podemos escrever o c\u00f3digo de captura e limpeza dos dados. Exigimos que tudo esteja escrito em Python , R , ou Stata . Podem ser c\u00f3digo padr\u00e3o ou cadernos (Colab, Jupyter, Rmarkdown, etc). No exemplo da RAIS, temos todo o c\u00f3digo escrito em Stata para consulta aqui .","title":"4. Escrever c\u00f3digo de captura e limpeza de dados"},{"location":"colab_data/#captura","text":"Esse script baixa automaticamente todos os dados originais e os salva em /input . Esses dados podem estar dispon\u00edveis em portais ou links FTP, podem ser raspados de sites, entre outros.","title":"Captura"},{"location":"colab_data/#limpeza","text":"Esse script transforma os dados originais salvos em /input nos dados limpos prontos para serem subidos na BD+ salvos em /output , tudo baseado nas tabelas de arquitetura. Cada tabela limpa para produ\u00e7\u00e3o pode ser salva como um arquivo .csv \u00fanico ou, caso seja muito grande (e.g. acima de 100-200 mb), ser particionada no formato Hive em v\u00e1rios sub-arquivos .csv . Nossa recomenda\u00e7\u00e3o \u00e9 particionar tabelas por ano , mes , sigla_uf , ou no m\u00e1ximo por id_municipio . Para a tabela microdados_vinculos da RAIS n\u00f3s particionamos por ano e sigla_uf , com a estrutura de pastas sendo /microdados_vinculos/ano=YYYY/sigla_uf=XX . N\u00f3s j\u00e1 criamos fun\u00e7\u00f5es \u00fateis para limpeza nas nossas APIs de Python e R. Por exemplo, com o pacote basedosdados voc\u00ea pode ler tabelas muito grandes, particionar tabelas automaticamente, gerar certas vari\u00e1veis comuns (e.g. sigla_uf a partir de id_uf ), etc. Tudo nesse passo deve seguir nosso manual de estilo e as melhores pr\u00e1ticas de programa\u00e7\u00e3o .","title":"Limpeza"},{"location":"colab_data/#5-se-necessario-organizar-arquivos-auxiliares","text":"\u00c9 comum bases de dados serem distribu\u00eddas com arquivos auxiliares. Esses podem incluir notas t\u00e9cnicas, descri\u00e7\u00f5es de coleta e amostragem, etc. Para ajudar usu\u00e1rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em /extra/auxiliary_files . Fique \u00e0 vontade para estruturar sub-pastas como quiser l\u00e1 dentro. O que importa \u00e9 que fique claro o que s\u00e3o esses arquivos.","title":"5. Se necess\u00e1rio, organizar arquivos auxiliares"},{"location":"colab_data/#6-se-necessario-criar-tabela-dicionario","text":"\u00c9 tamb\u00e9m comum que bases de dados grandes sejam estruturadas de forma a precisar de um dicion\u00e1rio. Para poupar espa\u00e7o, vari\u00e1veis STRING s\u00e3o convertidas em vari\u00e1veis num\u00e9ricas com cada chave mapeando um valor \u00fanico. Muitas vezes, especialmente com bases antigas, h\u00e1 m\u00faltiplos dicion\u00e1rios em formatos Excel ou outros. Na Base dos Dados n\u00f3s unificamos tudo em um \u00fanico arquivo em formato .csv . Descrevemos nossas v\u00e1rias diretrizes para dicion\u00e1rios no nosso manual de estilo . Por exemplo: Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas). Para cada tabela, coluna e cobertura temporal, cada chave mapeia unicamente um valor. Chaves n\u00e3o podem ter valores nulos. Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais. Chaves n\u00e3o possuem zeros \u00e0 esquerda. Exemplo: 01 deveria ser 1 . Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc. No exemplo da RAIS n\u00f3s criamos um dicion\u00e1rio completo aqui .","title":"6. Se necess\u00e1rio, criar tabela dicion\u00e1rio"},{"location":"colab_data/#7-subir-arquiteturas-dados-e-arquivos-auxiliares-no-google-cloud","text":"Tudo pronto! Agora s\u00f3 falta subir as coisas para o Google Cloud e depois enviar para revis\u00e3o. Desenvolvemos um cliente basedosdados (dispon\u00edvel para linha de comando e Python por enquanto) para facilitar esse processo e indicar configura\u00e7\u00f5es b\u00e1sicas que devem ser preenchidas sobre os dados.","title":"7. Subir arquiteturas, dados e arquivos auxiliares no Google Cloud"},{"location":"colab_data/#configure-seu-projeto-no-google-cloud-e-um-bucket-no-google-storage","text":"Os dados v\u00e3o passar ao todo por 3 lugares no Google Cloud: Storage: local onde ser\u00e3o armazenados o arquivos (arquiteturas, dados, arquivos auxiliares). BigQuery: banco de dados do Google, dividido em 2 projetos/tipos de tabela: Staging: banco para teste e tratamento final do conjunto de dados Produ\u00e7\u00e3o: banco oficial de publica\u00e7\u00e3o dos dados ( basedosdados ! ou o seu mesmo caso queira reproduzir o ambiente) Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. Seguindo o passo-a-passo: Acesse o link e aceite o Termo de Servi\u00e7os do Google Cloud. Clique em Create Project/Criar Projeto - escolha um nome bacana para o seu projeto, ele ter\u00e1 tamb\u00e9m um Project ID que ser\u00e1 utilizado para configura\u00e7\u00e3o local. Depois de criado o projeto, v\u00e1 at\u00e9 a funcionalidade de Storage e crie uma pasta, seu bucket , para voc\u00ea subir os dados.","title":"Configure seu projeto no Google Cloud e um bucket no Google Storage"},{"location":"colab_data/#configure-o-cli-localmente-clone-nosso-repositorio-e-abra-uma-nova-branch","text":"No seu terminal: Instale nosso cliente: pip install basedosdados . Rode basedosdados config init e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Clone um fork do nosso reposit\u00f3rio localmente. D\u00ea um cd para a pasta local do reposit\u00f3rio e abra uma nova branch com git checkout -b [BRANCH_ID] . Todas as adi\u00e7\u00f5es e modifica\u00e7\u00f5es ser\u00e3o feitas nessa branch .","title":"Configure o CLI localmente, clone nosso reposit\u00f3rio e abra uma nova branch"},{"location":"colab_data/#suba-e-configure-uma-tabela-no-seu-bucket","text":"Aqui s\u00e3o dois passos: primeiro publicamos uma base e depois publicamos tabelas. Publique uma base. Rode o comando basedosdados dataset create [DATASET_ID] --raw_path '/[DATASET_ID]/input' --auxiliary_files_path '/[DATASET_ID]/extra/auxiliary_files' . Preencha os arquivos de configura\u00e7\u00e3o da base: README.md : informa\u00e7\u00f5es b\u00e1sicas da base de dados aparecendo no Github. dataset_config.yaml : informa\u00e7\u00f5es espec\u00edficas da base de dados. Rode o comando basedosdados dataset update [DATASET_ID] para atualizar a base com as configura\u00e7\u00f5es preenchidas. Publique uma tabela (ou v\u00e1rias!) dentro da base Rode o comando basedosdados table create [DATASET_ID] [TABLE_ID] --data_path '/[DATASET_ID]/output/[TABLE_ID]' --architecture_path '/[DATASET_ID]/extra/architecture/[TABLE_ID]' . Preencha os arquivos de configura\u00e7\u00e3o da tabela: /[TABLE_ID]/table_config.yaml : informa\u00e7\u00f5es espec\u00edficas da tabela. /[TABLE_ID]/publish.sql : aqui voc\u00ea pode fazer tratamentos finais na tabela staging em SQL para publica\u00e7\u00e3o. Exemplo: modificar a query para dar um JOIN em outra tabela da BD+ e selecionar vari\u00e1veis. Rode o comando basedosdados table publish [DATASET_ID] [TABLE_ID] para publicar a tabela em produ\u00e7\u00e3o. Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo. \u00c9 sempre bom abrir o console do BigQuery e rodar algumas queries para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom\u00e1ticos para facilitar esse processo no futuro.","title":"Suba e configure uma tabela no seu bucket"},{"location":"colab_data/#8-enviar-tudo-para-revisao","text":"Ufa, \u00e9 isso! Agora s\u00f3 resta enviar tudo para revis\u00e3o no reposit\u00f3rio da Base dos Dados. Crie os commits necess\u00e1rios e rode um git push origin [BRANCH_ID] . Depois \u00e9 s\u00f3 abrir um pull request (PR) no nosso reposit\u00f3rio. Temos passos manuais e autom\u00e1ticos de revis\u00e3o para dados e metadados. Pessoas do time da Base dos Dados entrar\u00e3o em contato com voc\u00ea para pedir mudan\u00e7as ou tirar d\u00favidas. Quando tudo estiver redondo n\u00f3s fazemos um merge e os dados s\u00e3o publicados na nossa plataforma.","title":"8. Enviar tudo para revis\u00e3o"},{"location":"colab_infrastructure/","text":"Infraestrutura na BD+ O time de infraestrutura \u00e9 respons\u00e1vel pelas ferramentas de ingest\u00e3o de dados, que englobam desde o upload de dados at\u00e9 a disponibiliza\u00e7\u00e3o de dados no ambiente de produ\u00e7\u00e3o (processo do azul ao verde); pelo acesso de dados atrav\u00e9s de pacotes em Python e R (canto inferior direito); e pelo website (canto superior esquerdo). Atualmente \u00e9 poss\u00edvel colaborar em todas as frentes, com destaque ao desenvolvimento dos pesos e contrapesos e atualiza\u00e7\u00e3o do site. Colaborando com a Infra Voc\u00ea pode contribuir para nossa infraestrutura de v\u00e1rias formas: Melhorando a documenta\u00e7\u00e3o Criando tutoriais e workshops Melhorando nossa API em R Melhorando nossa API em Python Melhorando nosso o UX do nosso site (React, CSS, HTML) Criando checagens autom\u00e1ticas de qualidade de dados e metadados (em Python) Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :) Qual o procedimento? Dar uma olhada nos issues abertos no nosso Github com o prefixo [infra] . Pegar um que te interesse e j\u00e1 pode come\u00e7ar a fazer! Criar issues novos com sugest\u00f5es de features e come\u00e7ar depois de ouvir uma avalia\u00e7\u00e3o do resto da comunidade.","title":"Infraestrutura"},{"location":"colab_infrastructure/#infraestrutura-na-bd","text":"O time de infraestrutura \u00e9 respons\u00e1vel pelas ferramentas de ingest\u00e3o de dados, que englobam desde o upload de dados at\u00e9 a disponibiliza\u00e7\u00e3o de dados no ambiente de produ\u00e7\u00e3o (processo do azul ao verde); pelo acesso de dados atrav\u00e9s de pacotes em Python e R (canto inferior direito); e pelo website (canto superior esquerdo). Atualmente \u00e9 poss\u00edvel colaborar em todas as frentes, com destaque ao desenvolvimento dos pesos e contrapesos e atualiza\u00e7\u00e3o do site.","title":"Infraestrutura na BD+"},{"location":"colab_infrastructure/#colaborando-com-a-infra","text":"Voc\u00ea pode contribuir para nossa infraestrutura de v\u00e1rias formas: Melhorando a documenta\u00e7\u00e3o Criando tutoriais e workshops Melhorando nossa API em R Melhorando nossa API em Python Melhorando nosso o UX do nosso site (React, CSS, HTML) Criando checagens autom\u00e1ticas de qualidade de dados e metadados (em Python) Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)","title":"Colaborando com a Infra"},{"location":"colab_infrastructure/#qual-o-procedimento","text":"Dar uma olhada nos issues abertos no nosso Github com o prefixo [infra] . Pegar um que te interesse e j\u00e1 pode come\u00e7ar a fazer! Criar issues novos com sugest\u00f5es de features e come\u00e7ar depois de ouvir uma avalia\u00e7\u00e3o do resto da comunidade.","title":"Qual o procedimento?"},{"location":"datathon_2021/","text":"Datathon BD 2021 \ud83c\udfb2 Tema: Como dados abertos podem contribuir para o desenvolvimento igualit\u00e1rio no Brasil? A divulga\u00e7\u00e3o de dados p\u00fablicos mostra em n\u00fameros a desigualdade em diferentes aspectos da sociedade e \u00e9 a partir disso que podemos come\u00e7ar a elaborar solu\u00e7\u00f5es e iniciativas para um desenvolvimento mais democr\u00e1tico. Por isso, inspirados(as) no tema do Open Data Day 2021, resolvemos abrir espa\u00e7o para programadores, jornalistas, pesquisadores e entusiastas de dados pensarem conosco como podemos identificar ou combater desigualdades no Brasil a partir de dados p\u00fablicos . E como come\u00e7ar? N\u00f3s demos o ponto de partida: a partir das mais de 30 bases p\u00fablicas que disponibilizamos tratadas e integradas para uso da sociedade no nosso datalake p\u00fablico (BD+) . Recebemos ao todo mais de 30 inscri\u00e7\u00f5es de diferentes p\u00fablicos, agradecemos a participa\u00e7\u00e3o de todas e todos! \u26a0\ufe0f Nenhuma das an\u00e1lises tem a inten\u00e7\u00e3o de trazer evid\u00eancias rigorosamente testadas sobre os temas abordados, mas sim explorar e abrir poss\u00edveis caminhos para pensarmos os mesmos. E os(as) vencedores(as) s\u00e3o... UFRJ Analytica Equipe: Erica Ferreira, Pedro Boechat, Pedro Borges e Rafael Ribeiro (graduandos e analistas/cientistas de dados) An\u00e1lise: De que forma diferen\u00e7as no acesso a uma educa\u00e7\u00e3o de qualidade se manifestam em diferentes regi\u00f5es do pa\u00eds? Para entender melhor sobre essa e outras perguntas levantadas quanto \u00e0 qualidade de ensino e investimento em educa\u00e7\u00e3o, eles utilizaram ao todo 4 bases disponibilizadas na BD+: Atlas do Desenvolvimento Humano (ADH) , \u00cdndice de Desenvolvimento da Educa\u00e7\u00e3o B\u00e1sica (Ideb) , Finan\u00e7as do Brasil (Finbra) e nossa base de diret\u00f3rios brasileiros , que liga diferentes identifica\u00e7\u00f5es para munic\u00edpios, estados e regi\u00f5es do pa\u00eds. \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado Felipe Macedo Dias (graduando em Economia na USP) An\u00e1lise: A infla\u00e7\u00e3o foi maior em munic\u00edpios mais beneficiados pelo Aux\u00edlio Emergencial? O aux\u00edlio emergencial foi um grande tema socioecon\u00f4mico no contexto de pandemia no pa\u00eds, trazendo o debate tamb\u00e9m para sua perman\u00eancia em momentos posteriores. Felipe buscou uma explorar essa quest\u00e3o incialmente observando as capitais: existe alguma correla\u00e7\u00e3o entre o aumento dos pre\u00e7os, com um maior n\u00edvel de consumo, e a incid\u00eancia do aux\u00edlio nas capitais? Para isso, se utilizou das bases de popula\u00e7\u00e3o estimada do IBGE , Produto Interno Bruto (PIB) e dados do aux\u00edlio emergencial por munic\u00edpio . \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado","title":"1\u00aa edi\u00e7\u00e3o (mar/2021)"},{"location":"datathon_2021/#datathon-bd-2021","text":"","title":"Datathon BD 2021 \ud83c\udfb2"},{"location":"datathon_2021/#tema-como-dados-abertos-podem-contribuir-para-o-desenvolvimento-igualitario-no-brasil","text":"A divulga\u00e7\u00e3o de dados p\u00fablicos mostra em n\u00fameros a desigualdade em diferentes aspectos da sociedade e \u00e9 a partir disso que podemos come\u00e7ar a elaborar solu\u00e7\u00f5es e iniciativas para um desenvolvimento mais democr\u00e1tico. Por isso, inspirados(as) no tema do Open Data Day 2021, resolvemos abrir espa\u00e7o para programadores, jornalistas, pesquisadores e entusiastas de dados pensarem conosco como podemos identificar ou combater desigualdades no Brasil a partir de dados p\u00fablicos . E como come\u00e7ar? N\u00f3s demos o ponto de partida: a partir das mais de 30 bases p\u00fablicas que disponibilizamos tratadas e integradas para uso da sociedade no nosso datalake p\u00fablico (BD+) . Recebemos ao todo mais de 30 inscri\u00e7\u00f5es de diferentes p\u00fablicos, agradecemos a participa\u00e7\u00e3o de todas e todos! \u26a0\ufe0f Nenhuma das an\u00e1lises tem a inten\u00e7\u00e3o de trazer evid\u00eancias rigorosamente testadas sobre os temas abordados, mas sim explorar e abrir poss\u00edveis caminhos para pensarmos os mesmos.","title":"Tema: Como dados abertos podem contribuir para o desenvolvimento igualit\u00e1rio no Brasil?"},{"location":"datathon_2021/#e-osas-vencedoresas-sao","text":"UFRJ Analytica Equipe: Erica Ferreira, Pedro Boechat, Pedro Borges e Rafael Ribeiro (graduandos e analistas/cientistas de dados) An\u00e1lise: De que forma diferen\u00e7as no acesso a uma educa\u00e7\u00e3o de qualidade se manifestam em diferentes regi\u00f5es do pa\u00eds? Para entender melhor sobre essa e outras perguntas levantadas quanto \u00e0 qualidade de ensino e investimento em educa\u00e7\u00e3o, eles utilizaram ao todo 4 bases disponibilizadas na BD+: Atlas do Desenvolvimento Humano (ADH) , \u00cdndice de Desenvolvimento da Educa\u00e7\u00e3o B\u00e1sica (Ideb) , Finan\u00e7as do Brasil (Finbra) e nossa base de diret\u00f3rios brasileiros , que liga diferentes identifica\u00e7\u00f5es para munic\u00edpios, estados e regi\u00f5es do pa\u00eds. \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado Felipe Macedo Dias (graduando em Economia na USP) An\u00e1lise: A infla\u00e7\u00e3o foi maior em munic\u00edpios mais beneficiados pelo Aux\u00edlio Emergencial? O aux\u00edlio emergencial foi um grande tema socioecon\u00f4mico no contexto de pandemia no pa\u00eds, trazendo o debate tamb\u00e9m para sua perman\u00eancia em momentos posteriores. Felipe buscou uma explorar essa quest\u00e3o incialmente observando as capitais: existe alguma correla\u00e7\u00e3o entre o aumento dos pre\u00e7os, com um maior n\u00edvel de consumo, e a incid\u00eancia do aux\u00edlio nas capitais? Para isso, se utilizou das bases de popula\u00e7\u00e3o estimada do IBGE , Produto Interno Bruto (PIB) e dados do aux\u00edlio emergencial por munic\u00edpio . \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado","title":"E os(as) vencedores(as) s\u00e3o..."},{"location":"odd_2021/","text":"Open Data Day: Datathon BD Que tal explorar dados p\u00fablicos com a gente neste Dia dos Dados Abertos? Em parceria \u00e0 Escola de Matem\u00e1tica Aplicada da FGV , a Base dos Dados trar\u00e1 um evento apresentando sobre a nossa iniciativa, o novo curso de forma\u00e7\u00e3o em Ci\u00eancia de Dados da FGV EMAp e exemplos de aplica\u00e7\u00f5es com nosso data lake p\u00fablico. Para fechar o evento com chave de ouro, lan\u00e7aremos nosso primeiro de muitos Datathons! Por que participar? Essa \u00e9 uma \u00f3tima oportunidade de aprender mais sobre dados p\u00fablicos e conhecer pessoas interessantes! As melhores an\u00e1lises ser\u00e3o publicadas no nosso site e nas redes. A Base dos Dados tem a miss\u00e3o de universalizar o acesso a dados no Brasil. Coletamos bases que s\u00e3o p\u00fablicas mas s\u00e3o dif\u00edceis de manusear e organizar, e criamos uma forma super f\u00e1cil de us\u00e1-las. Acreditamos que a dist\u00e2ncia entre voc\u00ea e uma an\u00e1lise de dados deveria ser apenas uma boa pergunta. Fazemos tudo isso por meio de um mecanismo de busca, com 900+ bases catalogadas; e nosso data lake p\u00fablico que j\u00e1 tem mais de 30 bases p\u00fablicas tratadas, integradas e prontas para uso. O trabalho \u00e9 volunt\u00e1rio e mantido por nossa comunidade, formada por programadores, pesquisadores e entusiastas de dados. A FGV EMAp atua desde 2011 com os cursos de forma\u00e7\u00e3o para o desenvolvimento de uma matem\u00e1tica contempor\u00e2nea, adaptada aos desafios da era da informa\u00e7\u00e3o e do conhecimento. Em 2020 lan\u00e7aram o mais competo curso de Ci\u00eancia de Dados do pa\u00eds , um curso criado para transformar o horizonte profissional dos estudantes brasileiros interessados em seguir uma carreira inovadora. Como participar? O evento \u00e9 aberto a todos(as) e n\u00e3o \u00e9 necess\u00e1ria inscri\u00e7\u00e3o, basta entrar no nosso canal do Discord . Para participar do Datathon que ser\u00e1 lan\u00e7ado no dia do evento, inscreva-se em: https://forms.gle/k1ux95S5zR6oJ5RL8 Cronograma 14H Abertura: Base dos Dados e FGV EMAp 14:40 Workshop: Analisando dados p\u00fablicos na BD+ 15:40 Lan\u00e7amento do Datathon BD+ Todo o evento ir\u00e1 acontecer no nosso canal do Discord . O que \u00e9 o Dia dos Dados Abertos? O Dia dos Dados Abertos \u00e9 uma celebra\u00e7\u00e3o anual dos dados abertos em todo o mundo. Grupos de todas as partes do mundo criaram eventos locais no dia em que usar\u00e3o dados abertos em suas comunidades. \u00c9 uma oportunidade para mostrar os benef\u00edcios dos dados abertos e encorajar a ado\u00e7\u00e3o de pol\u00edticas de dados abertos no governo, empresas e na sociedade civil. Todos os resultados s\u00e3o abertos para que todos usem e reusem. Veja mais sobre o Dia de Dados Abertos.","title":"ODD (mar/2021)"},{"location":"odd_2021/#open-data-day-datathon-bd","text":"Que tal explorar dados p\u00fablicos com a gente neste Dia dos Dados Abertos? Em parceria \u00e0 Escola de Matem\u00e1tica Aplicada da FGV , a Base dos Dados trar\u00e1 um evento apresentando sobre a nossa iniciativa, o novo curso de forma\u00e7\u00e3o em Ci\u00eancia de Dados da FGV EMAp e exemplos de aplica\u00e7\u00f5es com nosso data lake p\u00fablico. Para fechar o evento com chave de ouro, lan\u00e7aremos nosso primeiro de muitos Datathons!","title":"Open Data Day: Datathon BD"},{"location":"odd_2021/#por-que-participar","text":"Essa \u00e9 uma \u00f3tima oportunidade de aprender mais sobre dados p\u00fablicos e conhecer pessoas interessantes! As melhores an\u00e1lises ser\u00e3o publicadas no nosso site e nas redes. A Base dos Dados tem a miss\u00e3o de universalizar o acesso a dados no Brasil. Coletamos bases que s\u00e3o p\u00fablicas mas s\u00e3o dif\u00edceis de manusear e organizar, e criamos uma forma super f\u00e1cil de us\u00e1-las. Acreditamos que a dist\u00e2ncia entre voc\u00ea e uma an\u00e1lise de dados deveria ser apenas uma boa pergunta. Fazemos tudo isso por meio de um mecanismo de busca, com 900+ bases catalogadas; e nosso data lake p\u00fablico que j\u00e1 tem mais de 30 bases p\u00fablicas tratadas, integradas e prontas para uso. O trabalho \u00e9 volunt\u00e1rio e mantido por nossa comunidade, formada por programadores, pesquisadores e entusiastas de dados. A FGV EMAp atua desde 2011 com os cursos de forma\u00e7\u00e3o para o desenvolvimento de uma matem\u00e1tica contempor\u00e2nea, adaptada aos desafios da era da informa\u00e7\u00e3o e do conhecimento. Em 2020 lan\u00e7aram o mais competo curso de Ci\u00eancia de Dados do pa\u00eds , um curso criado para transformar o horizonte profissional dos estudantes brasileiros interessados em seguir uma carreira inovadora.","title":"Por que participar?"},{"location":"odd_2021/#como-participar","text":"O evento \u00e9 aberto a todos(as) e n\u00e3o \u00e9 necess\u00e1ria inscri\u00e7\u00e3o, basta entrar no nosso canal do Discord . Para participar do Datathon que ser\u00e1 lan\u00e7ado no dia do evento, inscreva-se em: https://forms.gle/k1ux95S5zR6oJ5RL8","title":"Como participar?"},{"location":"odd_2021/#cronograma","text":"14H Abertura: Base dos Dados e FGV EMAp 14:40 Workshop: Analisando dados p\u00fablicos na BD+ 15:40 Lan\u00e7amento do Datathon BD+","title":"Cronograma"},{"location":"odd_2021/#todo-o-evento-ira-acontecer-no-nosso-canal-do-discord","text":"","title":"Todo o evento ir\u00e1 acontecer no nosso canal do Discord."},{"location":"odd_2021/#o-que-e-o-dia-dos-dados-abertos","text":"O Dia dos Dados Abertos \u00e9 uma celebra\u00e7\u00e3o anual dos dados abertos em todo o mundo. Grupos de todas as partes do mundo criaram eventos locais no dia em que usar\u00e3o dados abertos em suas comunidades. \u00c9 uma oportunidade para mostrar os benef\u00edcios dos dados abertos e encorajar a ado\u00e7\u00e3o de pol\u00edticas de dados abertos no governo, empresas e na sociedade civil. Todos os resultados s\u00e3o abertos para que todos usem e reusem. Veja mais sobre o Dia de Dados Abertos.","title":"O que \u00e9 o Dia dos Dados Abertos?"},{"location":"reference_api_cli/","text":"CLI Esta API \u00e9 composta de comandos tanto para gerenciamento de dados no Google Cloud quanto para requisi\u00e7\u00e3o de dados ( download e informa\u00e7\u00f5es de metadados) na sua m\u00e1quina local. Os comandos dispon\u00edveis dentro de cli atualmente s\u00e3o: Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"CLI"},{"location":"reference_api_cli/#cli","text":"Esta API \u00e9 composta de comandos tanto para gerenciamento de dados no Google Cloud quanto para requisi\u00e7\u00e3o de dados ( download e informa\u00e7\u00f5es de metadados) na sua m\u00e1quina local. Os comandos dispon\u00edveis dentro de cli atualmente s\u00e3o: Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"CLI"},{"location":"reference_api_py/","text":"Python Esta API \u00e9 composta por fun\u00e7\u00f5es com 2 tipos de funcionalidade: M\u00f3dulos para requisi\u00e7\u00e3o de dados : para aquele(as) que desejam somente consultar os dados e metadados do nosso projeto (ou qualquer outro projeto no Google Cloud). Classes para gerenciamento de dados no Google Cloud: para aqueles(as) que desejam subir dados no nosso projeto (ou qualquer outro projeto no Google Cloud, seguindo a nossa metodologia e infraestrutura). Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas M\u00f3dulos (Requisi\u00e7\u00e3o de dados) download ( savepath , query = None , dataset_id = None , table_id = None , query_project_id = 'basedosdados' , billing_project_id = None , limit = None , from_file = False , reauth = False , ** pandas_kwargs ) Download table or query result from basedosdados BigQuery (or other). Using a query : download('select * from basedosdados.br_suporte.diretorio_municipios limit 10') Using dataset_id & table_id : download(dataset_id='br_suporte', table_id='diretorio_municipios') You can also add arguments to modify save parameters: download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|') Parameters: Name Type Description Default savepath str, pathlib.PosixPath If savepath is a folder, it saves a file as savepath / table_id.csv or savepath / query_result.csv if table_id not available. If savepath is a file, saves data to file. required query str Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. None dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None limit int Optional Number of rows. None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False pandas_kwargs Extra arguments accepted by pandas.to_csv {} Exceptions: Type Description Exception If either table_id or dataset_id were are empty. Source code in basedosdados/download/download.py def download ( savepath , query = None , dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , from_file = False , reauth = False , ** pandas_kwargs , ): \"\"\"Download table or query result from basedosdados BigQuery (or other). * Using a **query**: `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')` * Using **dataset_id & table_id**: `download(dataset_id='br_suporte', table_id='diretorio_municipios')` You can also add arguments to modify save parameters: `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')` Args: savepath (str, pathlib.PosixPath): If savepath is a folder, it saves a file as `savepath / table_id.csv` or `savepath / query_result.csv` if table_id not available. If savepath is a file, saves data to file. query (str): Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard limit (int): Optional Number of rows. reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. pandas_kwargs (): Extra arguments accepted by [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) Raises: Exception: If either table_id or dataset_id were are empty. \"\"\" savepath = Path ( savepath ) # make sure that path exists if savepath . is_dir (): savepath . mkdir ( parents = True , exist_ok = True ) else : savepath . parent . mkdir ( parents = True , exist_ok = True ) if ( dataset_id is not None ) and ( table_id is not None ): table = read_table ( dataset_id , table_id , query_project_id = query_project_id , billing_project_id = billing_project_id , limit = limit , reauth = reauth , from_file = from_file , ) elif query is not None : query += f \" limit { limit } \" if limit is not None else \"\" table = read_sql ( query , billing_project_id = billing_project_id , from_file = from_file , reauth = reauth , ) else : raise BaseDosDadosException ( \"Either table_id, dataset_id or query should be filled.\" ) if savepath . is_dir (): if table_id is not None : savepath = savepath / ( table_id + \".csv\" ) else : savepath = savepath / ( \"query_result.csv\" ) pandas_kwargs [ \"index\" ] = pandas_kwargs . get ( \"index\" , False ) table . to_csv ( savepath , ** pandas_kwargs ) get_dataset_description ( dataset_id = None , query_project_id = 'basedosdados' , from_file = False ) Prints the full dataset description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Source code in basedosdados/download/download.py def get_dataset_description ( dataset_id = None , query_project_id = \"basedosdados\" , from_file = False ): \"\"\"Prints the full dataset description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) print ( dataset . description ) return None get_table_columns ( dataset_id = None , table_id = None , query_project_id = 'basedosdados' , from_file = False ) Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Examples: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) Source code in basedosdados/download/download.py def get_table_columns ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , from_file = False , ): \"\"\"Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Example: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) table_ref = client . get_table ( f \" { dataset_id } . { table_id } \" ) columns = [ ( field . name , field . field_type , field . description ) for field in table_ref . schema ] description = pd . DataFrame ( columns , columns = [ \"name\" , \"field_type\" , \"description\" ]) _print_output ( description ) return None get_table_description ( dataset_id = None , table_id = None , query_project_id = 'basedosdados' , from_file = False ) Prints the full table description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Source code in basedosdados/download/download.py def get_table_description ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , from_file = False , ): \"\"\"Prints the full table description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) table = client . get_table ( f \" { dataset_id } . { table_id } \" ) print ( table . description ) return None get_table_size ( dataset_id , table_id , billing_project_id , query_project_id = 'basedosdados' , from_file = False ) Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard required Examples: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) Source code in basedosdados/download/download.py def get_table_size ( dataset_id , table_id , billing_project_id , query_project_id = \"basedosdados\" , from_file = False , ): \"\"\"Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard Example: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) \"\"\" billing_client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = billing_project_id ) query = f \"\"\"SELECT COUNT(*) FROM { query_project_id } . { dataset_id } . { table_id } \"\"\" job = billing_client . query ( query , location = \"US\" ) num_rows = job . to_dataframe () . loc [ 0 , \"f0_\" ] size_mb = round ( job . total_bytes_processed / 1024 / 1024 , 2 ) table_data = pd . DataFrame ( [ { \"project_id\" : query_project_id , \"dataset_id\" : dataset_id , \"table_id\" : table_id , \"num_rows\" : num_rows , \"size_mb\" : size_mb , } ] ) _print_output ( table_data ) return None list_dataset_tables ( dataset_id , query_project_id = 'basedosdados' , from_file = False , filter_by = None , with_description = False ) Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in the table_id. None with_description bool Optional If True, fetch short table descriptions for each table that match the search criteria. False Examples: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) Source code in basedosdados/download/download.py def list_dataset_tables ( dataset_id , query_project_id = \"basedosdados\" , from_file = False , filter_by = None , with_description = False , ): \"\"\"Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in the table_id. with_description (bool): Optional If True, fetch short table descriptions for each table that match the search criteria. Example: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) tables_list = list ( client . list_tables ( dataset )) tables = pd . DataFrame ( [ table . table_id for table in tables_list ], columns = [ \"table_id\" ] ) if filter_by : tables = tables . loc [ tables [ \"table_id\" ] . str . contains ( filter_by )] if with_description : tables [ \"description\" ] = [ _get_header ( client . get_table ( f \" { dataset_id } . { table } \" ) . description ) for table in tables [ \"table_id\" ] ] _print_output ( tables ) return None list_datasets ( query_project_id = 'basedosdados' , filter_by = None , with_description = False , from_file = False ) Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Parameters: Name Type Description Default query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in dataset_id. None with_description bool Optional If True, fetch short dataset description for each dataset. False Examples: list_datasets( filter_by='sp', with_description=True, ) Source code in basedosdados/download/download.py def list_datasets ( query_project_id = \"basedosdados\" , filter_by = None , with_description = False , from_file = False , ): \"\"\"Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Args: query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in dataset_id. with_description (bool): Optional If True, fetch short dataset description for each dataset. Example: list_datasets( filter_by='sp', with_description=True, ) \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) datasets_list = list ( client . list_datasets ()) datasets = pd . DataFrame ( [ dataset . dataset_id for dataset in datasets_list ], columns = [ \"dataset_id\" ] ) if filter_by : datasets = datasets . loc [ datasets [ \"dataset_id\" ] . str . contains ( filter_by )] if with_description : datasets [ \"description\" ] = [ _get_header ( client . get_dataset ( dataset ) . description ) for dataset in datasets [ \"dataset_id\" ] ] _print_output ( datasets ) return None read_sql ( query , billing_project_id = None , from_file = False , reauth = False ) Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Parameters: Name Type Description Default query sql Valid SQL Standard Query to basedosdados required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_sql ( query , billing_project_id = None , from_file = False , reauth = False ): \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Args: query (sql): Valid SQL Standard Query to basedosdados billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. Returns: pd.DataFrame: Query result \"\"\" try : # Set a two hours timeout bigquery_storage_v1 . client . BigQueryReadClient . read_rows = partialmethod ( bigquery_storage_v1 . client . BigQueryReadClient . read_rows , timeout = 3600 * 2 , ) return pandas_gbq . read_gbq ( query , credentials = credentials ( from_file = from_file , reauth = reauth ), project_id = billing_project_id , ) except ( OSError , ValueError ) as e : msg = ( \" \\n We are not sure which Google Cloud project should be billed. \\n \" \"First, you should make sure that you have a Google Cloud project. \\n \" \"If you don't have one, set one up following these steps: \\n \" \" \\t 1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \" \\t 2. Agree with Terms of Service if asked \\n \" \" \\t 3. Click in Create Project \\n \" \" \\t 4. Put a cool name in your project \\n \" \" \\t 5. Hit create \\n \" \" \\n \" \"Copy the Project ID, (notice that it is not the Project Name) \\n \" \"Now, you have two options: \\n \" \"1. Add an argument to your function poiting to the billing project id. \\n \" \" Like `bd.read_table('br_ibge_pib', 'municipios', billing_project_id=<YOUR_PROJECT_ID>)` \\n \" \"2. You can set a project_id in the environment by running the following command in your terminal: `gcloud config set project <YOUR_PROJECT_ID>`. \\n \" \" Bear in mind that you need `gcloud` installed.\" ) raise BaseDosDadosException ( msg ) from e except GenericGBQException as e : if \"Reason: 403\" in str ( e ): raise BaseDosDadosException ( \" \\n You still don't have a Google Cloud Project. \\n \" \"Set one up following these steps: \\n \" \"1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \"2. Agree with Terms of Service if asked \\n \" \"3. Click in Create Project \\n \" \"4. Put a cool name in your project \\n \" \"5. Hit create \\n \" \"6. Rerun this command with the flag `reauth=True`. \\n \" \" Like `read_table('br_ibge_pib', 'municipios', reauth=True)`\" ) raise read_table ( dataset_id , table_id , query_project_id = 'basedosdados' , billing_project_id = None , limit = None , from_file = False , reauth = False ) Load data from BigQuery using dataset_id and table_id. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False limit int Optional. Number of rows to read from table. None Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_table ( dataset_id , table_id , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , from_file = False , reauth = False , ): \"\"\"Load data from BigQuery using dataset_id and table_id. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. limit (int): Optional. Number of rows to read from table. Returns: pd.DataFrame: Query result \"\"\" if ( dataset_id is not None ) and ( table_id is not None ): query = f \"\"\" SELECT * FROM ` { query_project_id } . { dataset_id } . { table_id } `\"\"\" if limit is not None : query += f \" LIMIT { limit } \" else : raise BaseDosDadosException ( \"Both table_id and dataset_id should be filled.\" ) return read_sql ( query , billing_project_id = billing_project_id , from_file = from_file , reauth = reauth ) Classes (Gerenciamento de dados) Storage Manage files on Google Cloud Storage. copy_table ( self , source_bucket_name = 'basedosdados' , destination_bucket_name = None , mode = 'staging' ) Copies table from a source bucket to your bucket, sends request in batches. Parameters: Name Type Description Default source_bucket_name str The bucket name from which to copy data. You can change it to copy from other external bucket. 'basedosdados' destination_bucket_name str Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) None mode str Optional Folder of which dataset to update. Defaults to \"staging\". 'staging' Source code in basedosdados/upload/storage.py def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name (str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Optional Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = list ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" ) ) if source_table_ref == []: raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) # Divides source_table_ref list for maximum batch request size source_table_ref_chunks = [ source_table_ref [ i : i + 999 ] for i in range ( 0 , len ( source_table_ref ), 999 ) ] for source_table in source_table_ref_chunks : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket ) delete_file ( self , filename , mode , partitions = None , not_found_ok = False ) Deletes file from path <bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename> . Parameters: Name Type Description Default filename str Name of the file to be deleted required mode str Folder of which dataset to update [raw|staging|all] required partitions str, pathlib.PosixPath, or dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None not_found_ok bool Optional. What to do if file not found False Source code in basedosdados/upload/storage.py def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) mode = [ \"raw\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists () or not blob . exists () and not not_found_ok : blob . delete () else : return delete_table ( self , mode = 'staging' , bucket_name = None , not_found_ok = False ) Deletes a table from storage, sends request in batches. Parameters: Name Type Description Default mode str Optional Folder of which dataset to update. 'staging' bucket_name str The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) None not_found_ok bool Optional. What to do if table not found False Source code in basedosdados/upload/storage.py def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. Args: mode (str): Optional Folder of which dataset to update. bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = list ( self . client [ \"storage_staging\" ] . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) ) else : table_blobs = list ( self . bucket . list_blobs ( prefix = prefix )) if table_blobs == []: if not_found_ok : return else : raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) else : # Divides table_blobs list for maximum batch request size table_blobs_chunks = [ table_blobs [ i : i + 999 ] for i in range ( 0 , len ( table_blobs ), 999 ) ] for source_table in table_blobs_chunks : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : blob . delete () download ( self , filename = '*' , savepath = '' , partitions = None , mode = 'raw' , if_not_exists = 'raise' ) Download files from Google Storage from path mode / dataset_id / table_id / partitions / filename and replicate folder hierarchy on save, There are 2 modes: * raw : download file from raw mode * staging : download file from staging mode You can also use the partitions argument to choose files from a partition Parameters: Name Type Description Default filename str Optional Specify which file to download. If \" \" , downloads all files within the bucket folder. Defaults to \" \". '*' savepath str Where you want to save the data on your computer. Must be a path to a directory. '' partitions str, dict Optional If downloading a single file, use this to specify the partition path from which to download. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None mode str Optional Folder of which dataset to update.[raw/staging] 'raw' if_not_exists str Optional. What to do if data not found. 'raise' : Raises FileNotFoundError. 'pass' : Do nothing and exit the function 'raise' Exceptions: Type Description FileNotFoundError If the given path <mode>/<dataset_id>/<table_id>/<partitions>/<filename> could not be found or there are no files to download. Source code in basedosdados/upload/storage.py def download ( self , filename = \"*\" , savepath = \"\" , partitions = None , mode = \"raw\" , if_not_exists = \"raise\" , ): \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy on save, There are 2 modes: * `raw`: download file from raw mode * `staging`: download file from staging mode You can also use the `partitions` argument to choose files from a partition Args: filename (str): Optional Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\". savepath (str): Where you want to save the data on your computer. Must be a path to a directory. partitions (str, dict): Optional If downloading a single file, use this to specify the partition path from which to download. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` mode (str): Optional Folder of which dataset to update.[raw/staging] if_not_exists (str): Optional. What to do if data not found. * 'raise' : Raises FileNotFoundError. * 'pass' : Do nothing and exit the function Raises: FileNotFoundError: If the given path `<mode>/<dataset_id>/<table_id>/<partitions>/<filename>` could not be found or there are no files to download. \"\"\" # Prefix to locate files within the bucket prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # Add specific partition to search prefix if partitions : prefix += self . _resolve_partitions ( partitions ) # if no filename is passed, list all blobs within a given table if filename != \"*\" : prefix += filename blob_list = list ( self . bucket . list_blobs ( prefix = prefix )) # if there are no blobs matching the search raise FileNotFoundError or return if blob_list == []: if if_not_exists == \"raise\" : raise FileNotFoundError ( f \"Could not locate files at { prefix } \" ) else : return # download all blobs matching the search to given savepath for blob in blob_list : # parse blob.name and get the csv file name csv_name = blob . name . split ( \"/\" )[ - 1 ] # build folder path replicating storage hierarchy blob_folder = blob . name . replace ( csv_name , \"\" ) # replicate folder hierarchy ( Path ( savepath ) / blob_folder ) . mkdir ( parents = True , exist_ok = True ) # download blob to savepath blob . download_to_filename ( filename = f \" { savepath } / { blob . name } \" ) init ( self , replace = False , very_sure = False ) Initializes bucket and folders. Folder should be: raw : that contains really raw data staging : preprocessed data ready to upload to BigQuery Parameters: Name Type Description Default replace bool Optional. Whether to replace if bucket already exists False very_sure bool Optional. Are you aware that everything is going to be erased if you replace the bucket? False Exceptions: Type Description Warning very_sure argument is still False. Source code in basedosdados/upload/storage.py def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) else : self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" ) upload ( self , path , mode = 'all' , partitions = None , if_exists = 'raise' , ** upload_args ) Upload to storage at <bucket_name>/<mode>/<dataset_id>/<table_id> . You can: Add a single file setting path = <file_path> . Add a folder with multiple files setting path = <folder_path> . The folder should just contain the files and no folders. Add partitioned files setting path = <folder_path> . This folder must follow the hive partitioning scheme i.e. <table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv (ex: mytable/country=brasil/year=2020/mypart.csv ). Remember all files must follow a single schema. Otherwise, things might fail in the future. There are 3 modes: raw : should contain raw files from datasource staging : should contain pre-treated files ready to upload to BiqQuery all : if no treatment is needed, use all . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file or folder that you want to upload to storage required mode str Folder of which dataset to update [raw|staging|all] 'all' partitions str, pathlib.PosixPath, or dict Optional. If adding a single file , use this to add it to a specific partition. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str Optional. What to do if data exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' upload_args Extra arguments accepted by google.cloud.storage.blob.Blob.upload_from_file {} Source code in basedosdados/upload/storage.py def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 3 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) mode = [ \"raw\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists != \"pass\" : raise Exception ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"Set if_exists to 'replace' to overwrite data\" ) Dataset Manage datasets in BigQuery. create ( self , mode = 'all' , if_exists = 'raise' ) Creates BigQuery datasets given dataset_id . It can create two datasets: <dataset_id> (mode = 'prod') <dataset_id>_staging (mode = 'staging') If mode is all, it creates both. Parameters: Name Type Description Default mode str Optional. Which dataset to create [prod|staging|all]. 'all' if_exists str Optional. What to do if dataset exists raise : Raises Conflic exception replace : Drop all tables and replace dataset update : Update dataset description pass : Do nothing 'raise' Exceptions: Type Description Warning Dataset already exists and if_exists is set to raise Source code in basedosdados/upload/dataset.py def create ( self , mode = \"all\" , if_exists = \"raise\" ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflic exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ]) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : job = m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. except Conflict : if if_exists == \"pass\" : return else : raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) # Make prod dataset public self . publicize () delete ( self , mode = 'all' ) Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Parameters: Name Type Description Default mode str Optional. Which dataset to delete [prod|staging|all] 'all' Source code in basedosdados/upload/dataset.py def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True ) init ( self , replace = False ) Initialize dataset folder at metadata_path at metadata_path/<dataset_id> . The folder should contain: dataset_config.yaml README.md Parameters: Name Type Description Default replace str Optional. Whether to replace existing folder. False Exceptions: Type Description FileExistsError If dataset folder already exists and replace is False Source code in basedosdados/upload/dataset.py def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) for file in ( Path ( self . templates ) / \"dataset\" ) . glob ( \"*\" ): if file . name in [ \"dataset_config.yaml\" , \"README.md\" ]: # Load and fill template template = self . _render_template ( f \"dataset/ { file . name } \" , dict ( dataset_id = self . dataset_id ) ) # Write file ( self . dataset_folder / file . name ) . open ( \"w\" , encoding = \"utf-8\" ) . write ( template ) # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self publicize ( self , mode = 'all' ) Changes IAM configuration to turn BigQuery dataset public. Parameters: Name Type Description Default mode bool Which dataset to create [prod|staging|all]. 'all' Source code in basedosdados/upload/dataset.py def publicize ( self , mode = \"all\" ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ]) update ( self , mode = 'all' ) Update dataset description. Toogle mode to choose which dataset to update. Parameters: Name Type Description Default mode str Optional. Which dataset to update [prod|staging|all] 'all' Source code in basedosdados/upload/dataset.py def update ( self , mode = \"all\" ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. dataset = m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ]), fields = [ \"description\" ] ) # Make an API request. Table Manage tables in Google Cloud Storage and BigQuery. append ( self , filepath , partitions = None , if_exists = 'raise' , ** upload_args ) Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Parameters: Name Type Description Default filepath str or pathlib.PosixPath Where to find the file that you want to upload to create a table with required partitions str, pathlib.PosixPath, dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str 0ptional. What to do if data with same name exists in storage 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Source code in basedosdados/upload/table.py def append ( self , filepath , partitions = None , if_exists = \"raise\" , ** upload_args ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing \"\"\" Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = None , if_exists = if_exists , ** upload_args , ) self . create ( if_table_exists = \"replace\" , if_table_config_exists = \"pass\" , if_storage_data_exists = \"pass\" , ) create ( self , path = None , job_config_params = None , force_dataset = True , if_table_exists = 'raise' , if_storage_data_exists = 'raise' , if_table_config_exists = 'raise' , source_format = 'csv' , columns_config_url = None ) Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at <dataset_id>_staging.<table_id> in BigQuery. It looks for data saved in Storage at <bucket_name>/staging/<dataset_id>/<table_id>/* and builds the table. It currently supports the types: Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme <key1>=<value1>/<key2>=<value2> - for instance, year=2012/country=BR . The partition is automatcally detected by searching for partitions on the table_config.yaml . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file that you want to upload to create a table with None job_config_params dict Optional. Job configuration params from bigquery None if_table_exists str Optional What to do if table exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' force_dataset bool Creates <dataset_id> folder and BigQuery Dataset if it doesn't exists. True if_table_config_exists str Optional. What to do if config files already exist 'raise': Raises FileExistError 'replace': Replace with blank template 'pass'; Do nothing 'raise' if_storage_data_exists str Optional. What to do if data already exists on your bucket: 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. 'csv' columns_config_url str google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: \"coluna\" and column description: \"descricao\" None Source code in basedosdados/upload/table.py def create ( self , path = None , job_config_params = None , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url = None , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>` - for instance, `year=2012/country=BR`. The partition is automatcally detected by searching for `partitions` on the `table_config.yaml`. Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. columns_config_url (str): google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. The sheet must contain the column name: \"coluna\" and column description: \"descricao\" \"\"\" if path is None : # Look if table data already exists at Storage data = self . client [ \"storage_staging\" ] . list_blobs ( self . bucket_name , prefix = f \"staging/ { self . dataset_id } / { self . table_id } \" ) # Raise: Cannot create table without external data if not data : raise BaseDosDadosException ( \"You must provide a path for uploading data\" ) # Add data to storage if isinstance ( path , ( str , Path , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , columns_config_url = columns_config_url , ) table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = Datatype ( self , source_format , \"staging\" , partitioned = self . _is_partitioned () ) . external_config # Lookup if table alreay exists table_ref = None try : table_ref = self . client [ \"bigquery_staging\" ] . get_table ( self . table_full_name [ \"staging\" ] ) except google . api_core . exceptions . NotFound : pass if isinstance ( table_ref , google . cloud . bigquery . table . Table ): if if_table_exists == \"pass\" : return None elif if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table ) delete ( self , mode ) Deletes table in BigQuery. Parameters: Name Type Description Default mode str Table of which table to delete [prod|staging|all] required Source code in basedosdados/upload/table.py def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging|all] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True ) init ( self , data_sample_path = None , if_folder_exists = 'raise' , if_table_config_exists = 'raise' , source_format = 'csv' , columns_config_url = None ) Initialize table folder at metadata_path at metadata_path/<dataset_id>/<table_id> . The folder should contain: table_config.yaml publish.sql You can also point to a sample of the data to auto complete columns names. Parameters: Name Type Description Default data_sample_path str, pathlib.PosixPath Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. None if_folder_exists str Optional. What to do if table folder exists 'raise' : Raises FileExistsError 'replace' : Replace folder 'pass' : Do nothing 'raise' if_table_config_exists str Optional What to do if table_config.yaml and publish.sql exists 'raise' : Raises FileExistsError 'replace' : Replace files with blank template 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. 'csv' columns_config_url str google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: \"coluna\" and column description: \"descricao\" None Exceptions: Type Description FileExistsError If folder exists and replace is False. NotImplementedError If data sample is not in supported type or format. Source code in basedosdados/upload/table.py def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url = None , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing if_table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. columns_config_url (str): google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. The sheet must contain the column name: \"coluna\" and column description: \"descricao\" Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) elif if_folder_exists == \"pass\" : return self if not data_sample_path and if_table_config_exists != \"pass\" : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) partition_columns = [] if isinstance ( data_sample_path , ( str , Path , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] columns = Datatype ( self , source_format ) . header ( data_sample_path ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : # Check if config files exists before passing if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass # Raise if no sample to determine columns elif not data_sample_path : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) else : self . _make_template ( columns , partition_columns ) elif if_table_config_exists == \"raise\" : # Check if config files already exist if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) # if config files don't exist, create them else : self . _make_template ( columns , partition_columns ) else : # Raise: without a path to data sample, should not replace config files with empty template self . _make_template ( columns , partition_columns ) if columns_config_url is not None : self . update_columns ( columns_config_url ) return self publish ( self , if_exists = 'raise' ) Creates BigQuery table at production dataset. Table should be located at <dataset_id>.<table_id> . It creates a view that uses the query from <metadata_path>/<dataset_id>/<table_id>/publish.sql . Make sure that all columns from the query also exists at <metadata_path>/<dataset_id>/<table_id>/table_config.sql , including the partitions. Parameters: Name Type Description Default if_exists str Optional. What to do if table exists. 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Check if all required fields are filled Source code in basedosdados/upload/table.py def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" , encoding = \"utf-8\" ) . read () ) . result () self . update ( \"prod\" ) update ( self , mode = 'all' , not_found_ok = True ) Updates BigQuery schema and description. Parameters: Name Type Description Default mode str Optional. Table of which table to update [prod|staging|all] 'all' not_found_ok bool Optional. What to do if table is not found True Source code in basedosdados/upload/table.py def update ( self , mode = \"all\" , not_found_ok = True ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue # if m == \"staging\": table . description = self . _render_template ( Path ( \"table/table_description.txt\" ), self . table_config ) # save table description open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , encoding = \"utf-8\" , ) . write ( table . description ) if m == \"prod\" : table . schema = self . _load_schema ( m ) self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = [ \"description\" , \"schema\" ] ) update_columns ( self , columns_config_url ) Fills descriptions of tables automatically using a public google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: \"coluna\" and column description: \"descricao\" Parameters: Name Type Description Default columns_config_url str google sheets URL. required Source code in basedosdados/upload/table.py def update_columns ( self , columns_config_url ): \"\"\"Fills descriptions of tables automatically using a public google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. The sheet must contain the column name: \"coluna\" and column description: \"descricao\" Args: columns_config_url (str): google sheets URL. \"\"\" ruamel = ryaml . YAML () ruamel . preserve_quotes = True ruamel . indent ( mapping = 4 , sequence = 6 , offset = 4 ) table_config_yaml = ruamel . load ( ( self . table_folder / \"table_config.yaml\" ) . open () ) if ( \"edit#gid=\" not in columns_config_url or \"https://docs.google.com/spreadsheets/d/\" not in columns_config_url or not columns_config_url . split ( \"=\" )[ 1 ] . isdigit () ): raise Exception ( \"The Google sheet url not in correct format.\" \"The url must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>\" ) df = self . _sheet_to_df ( columns_config_url ) if \"coluna\" not in df . columns . tolist (): raise Exception ( \"Column 'coluna' not found in Google the google sheet. \" \"The sheet must contain the column name: 'coluna' and column description: 'descricao'\" ) elif \"descricao\" not in df . columns . tolist (): raise Exception ( \"Column 'descricao' not found in Google the google sheet. \" \"The sheet must contain the column name: 'coluna' and column description: 'descricao'\" ) columns_parameters = zip ( df [ \"coluna\" ] . tolist (), df [ \"descricao\" ] . tolist ()) for name , description in columns_parameters : for col in table_config_yaml [ \"columns\" ]: if col [ \"name\" ] == name : col [ \"description\" ] = description ruamel . dump ( table_config_yaml , stream = self . table_folder / \"table_config.yaml\" )","title":"Python"},{"location":"reference_api_py/#python","text":"Esta API \u00e9 composta por fun\u00e7\u00f5es com 2 tipos de funcionalidade: M\u00f3dulos para requisi\u00e7\u00e3o de dados : para aquele(as) que desejam somente consultar os dados e metadados do nosso projeto (ou qualquer outro projeto no Google Cloud). Classes para gerenciamento de dados no Google Cloud: para aqueles(as) que desejam subir dados no nosso projeto (ou qualquer outro projeto no Google Cloud, seguindo a nossa metodologia e infraestrutura). Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"Python"},{"location":"reference_api_py/#modulos-requisicao-de-dados","text":"","title":"M\u00f3dulos (Requisi\u00e7\u00e3o de dados)"},{"location":"reference_api_py/#basedosdados.download.download.download","text":"Download table or query result from basedosdados BigQuery (or other). Using a query : download('select * from basedosdados.br_suporte.diretorio_municipios limit 10') Using dataset_id & table_id : download(dataset_id='br_suporte', table_id='diretorio_municipios') You can also add arguments to modify save parameters: download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|') Parameters: Name Type Description Default savepath str, pathlib.PosixPath If savepath is a folder, it saves a file as savepath / table_id.csv or savepath / query_result.csv if table_id not available. If savepath is a file, saves data to file. required query str Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. None dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None limit int Optional Number of rows. None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False pandas_kwargs Extra arguments accepted by pandas.to_csv {} Exceptions: Type Description Exception If either table_id or dataset_id were are empty. Source code in basedosdados/download/download.py def download ( savepath , query = None , dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , from_file = False , reauth = False , ** pandas_kwargs , ): \"\"\"Download table or query result from basedosdados BigQuery (or other). * Using a **query**: `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')` * Using **dataset_id & table_id**: `download(dataset_id='br_suporte', table_id='diretorio_municipios')` You can also add arguments to modify save parameters: `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')` Args: savepath (str, pathlib.PosixPath): If savepath is a folder, it saves a file as `savepath / table_id.csv` or `savepath / query_result.csv` if table_id not available. If savepath is a file, saves data to file. query (str): Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard limit (int): Optional Number of rows. reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. pandas_kwargs (): Extra arguments accepted by [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) Raises: Exception: If either table_id or dataset_id were are empty. \"\"\" savepath = Path ( savepath ) # make sure that path exists if savepath . is_dir (): savepath . mkdir ( parents = True , exist_ok = True ) else : savepath . parent . mkdir ( parents = True , exist_ok = True ) if ( dataset_id is not None ) and ( table_id is not None ): table = read_table ( dataset_id , table_id , query_project_id = query_project_id , billing_project_id = billing_project_id , limit = limit , reauth = reauth , from_file = from_file , ) elif query is not None : query += f \" limit { limit } \" if limit is not None else \"\" table = read_sql ( query , billing_project_id = billing_project_id , from_file = from_file , reauth = reauth , ) else : raise BaseDosDadosException ( \"Either table_id, dataset_id or query should be filled.\" ) if savepath . is_dir (): if table_id is not None : savepath = savepath / ( table_id + \".csv\" ) else : savepath = savepath / ( \"query_result.csv\" ) pandas_kwargs [ \"index\" ] = pandas_kwargs . get ( \"index\" , False ) table . to_csv ( savepath , ** pandas_kwargs )","title":"download()"},{"location":"reference_api_py/#basedosdados.download.download.get_dataset_description","text":"Prints the full dataset description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Source code in basedosdados/download/download.py def get_dataset_description ( dataset_id = None , query_project_id = \"basedosdados\" , from_file = False ): \"\"\"Prints the full dataset description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) print ( dataset . description ) return None","title":"get_dataset_description()"},{"location":"reference_api_py/#basedosdados.download.download.get_table_columns","text":"Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Examples: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) Source code in basedosdados/download/download.py def get_table_columns ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , from_file = False , ): \"\"\"Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Example: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) table_ref = client . get_table ( f \" { dataset_id } . { table_id } \" ) columns = [ ( field . name , field . field_type , field . description ) for field in table_ref . schema ] description = pd . DataFrame ( columns , columns = [ \"name\" , \"field_type\" , \"description\" ]) _print_output ( description ) return None","title":"get_table_columns()"},{"location":"reference_api_py/#basedosdados.download.download.get_table_description","text":"Prints the full table description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Source code in basedosdados/download/download.py def get_table_description ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , from_file = False , ): \"\"\"Prints the full table description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) table = client . get_table ( f \" { dataset_id } . { table_id } \" ) print ( table . description ) return None","title":"get_table_description()"},{"location":"reference_api_py/#basedosdados.download.download.get_table_size","text":"Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard required Examples: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) Source code in basedosdados/download/download.py def get_table_size ( dataset_id , table_id , billing_project_id , query_project_id = \"basedosdados\" , from_file = False , ): \"\"\"Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard Example: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) \"\"\" billing_client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = billing_project_id ) query = f \"\"\"SELECT COUNT(*) FROM { query_project_id } . { dataset_id } . { table_id } \"\"\" job = billing_client . query ( query , location = \"US\" ) num_rows = job . to_dataframe () . loc [ 0 , \"f0_\" ] size_mb = round ( job . total_bytes_processed / 1024 / 1024 , 2 ) table_data = pd . DataFrame ( [ { \"project_id\" : query_project_id , \"dataset_id\" : dataset_id , \"table_id\" : table_id , \"num_rows\" : num_rows , \"size_mb\" : size_mb , } ] ) _print_output ( table_data ) return None","title":"get_table_size()"},{"location":"reference_api_py/#basedosdados.download.download.list_dataset_tables","text":"Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in the table_id. None with_description bool Optional If True, fetch short table descriptions for each table that match the search criteria. False Examples: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) Source code in basedosdados/download/download.py def list_dataset_tables ( dataset_id , query_project_id = \"basedosdados\" , from_file = False , filter_by = None , with_description = False , ): \"\"\"Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in the table_id. with_description (bool): Optional If True, fetch short table descriptions for each table that match the search criteria. Example: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) tables_list = list ( client . list_tables ( dataset )) tables = pd . DataFrame ( [ table . table_id for table in tables_list ], columns = [ \"table_id\" ] ) if filter_by : tables = tables . loc [ tables [ \"table_id\" ] . str . contains ( filter_by )] if with_description : tables [ \"description\" ] = [ _get_header ( client . get_table ( f \" { dataset_id } . { table } \" ) . description ) for table in tables [ \"table_id\" ] ] _print_output ( tables ) return None","title":"list_dataset_tables()"},{"location":"reference_api_py/#basedosdados.download.download.list_datasets","text":"Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Parameters: Name Type Description Default query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in dataset_id. None with_description bool Optional If True, fetch short dataset description for each dataset. False Examples: list_datasets( filter_by='sp', with_description=True, ) Source code in basedosdados/download/download.py def list_datasets ( query_project_id = \"basedosdados\" , filter_by = None , with_description = False , from_file = False , ): \"\"\"Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Args: query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in dataset_id. with_description (bool): Optional If True, fetch short dataset description for each dataset. Example: list_datasets( filter_by='sp', with_description=True, ) \"\"\" client = bigquery . Client ( credentials = credentials ( from_file = from_file ), project = query_project_id ) datasets_list = list ( client . list_datasets ()) datasets = pd . DataFrame ( [ dataset . dataset_id for dataset in datasets_list ], columns = [ \"dataset_id\" ] ) if filter_by : datasets = datasets . loc [ datasets [ \"dataset_id\" ] . str . contains ( filter_by )] if with_description : datasets [ \"description\" ] = [ _get_header ( client . get_dataset ( dataset ) . description ) for dataset in datasets [ \"dataset_id\" ] ] _print_output ( datasets ) return None","title":"list_datasets()"},{"location":"reference_api_py/#basedosdados.download.download.read_sql","text":"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Parameters: Name Type Description Default query sql Valid SQL Standard Query to basedosdados required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_sql ( query , billing_project_id = None , from_file = False , reauth = False ): \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Args: query (sql): Valid SQL Standard Query to basedosdados billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. Returns: pd.DataFrame: Query result \"\"\" try : # Set a two hours timeout bigquery_storage_v1 . client . BigQueryReadClient . read_rows = partialmethod ( bigquery_storage_v1 . client . BigQueryReadClient . read_rows , timeout = 3600 * 2 , ) return pandas_gbq . read_gbq ( query , credentials = credentials ( from_file = from_file , reauth = reauth ), project_id = billing_project_id , ) except ( OSError , ValueError ) as e : msg = ( \" \\n We are not sure which Google Cloud project should be billed. \\n \" \"First, you should make sure that you have a Google Cloud project. \\n \" \"If you don't have one, set one up following these steps: \\n \" \" \\t 1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \" \\t 2. Agree with Terms of Service if asked \\n \" \" \\t 3. Click in Create Project \\n \" \" \\t 4. Put a cool name in your project \\n \" \" \\t 5. Hit create \\n \" \" \\n \" \"Copy the Project ID, (notice that it is not the Project Name) \\n \" \"Now, you have two options: \\n \" \"1. Add an argument to your function poiting to the billing project id. \\n \" \" Like `bd.read_table('br_ibge_pib', 'municipios', billing_project_id=<YOUR_PROJECT_ID>)` \\n \" \"2. You can set a project_id in the environment by running the following command in your terminal: `gcloud config set project <YOUR_PROJECT_ID>`. \\n \" \" Bear in mind that you need `gcloud` installed.\" ) raise BaseDosDadosException ( msg ) from e except GenericGBQException as e : if \"Reason: 403\" in str ( e ): raise BaseDosDadosException ( \" \\n You still don't have a Google Cloud Project. \\n \" \"Set one up following these steps: \\n \" \"1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \"2. Agree with Terms of Service if asked \\n \" \"3. Click in Create Project \\n \" \"4. Put a cool name in your project \\n \" \"5. Hit create \\n \" \"6. Rerun this command with the flag `reauth=True`. \\n \" \" Like `read_table('br_ibge_pib', 'municipios', reauth=True)`\" ) raise","title":"read_sql()"},{"location":"reference_api_py/#basedosdados.download.download.read_table","text":"Load data from BigQuery using dataset_id and table_id. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False limit int Optional. Number of rows to read from table. None Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_table ( dataset_id , table_id , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , from_file = False , reauth = False , ): \"\"\"Load data from BigQuery using dataset_id and table_id. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. limit (int): Optional. Number of rows to read from table. Returns: pd.DataFrame: Query result \"\"\" if ( dataset_id is not None ) and ( table_id is not None ): query = f \"\"\" SELECT * FROM ` { query_project_id } . { dataset_id } . { table_id } `\"\"\" if limit is not None : query += f \" LIMIT { limit } \" else : raise BaseDosDadosException ( \"Both table_id and dataset_id should be filled.\" ) return read_sql ( query , billing_project_id = billing_project_id , from_file = from_file , reauth = reauth )","title":"read_table()"},{"location":"reference_api_py/#classes-gerenciamento-de-dados","text":"","title":"Classes (Gerenciamento de dados)"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage","text":"Manage files on Google Cloud Storage.","title":"Storage"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage.copy_table","text":"Copies table from a source bucket to your bucket, sends request in batches. Parameters: Name Type Description Default source_bucket_name str The bucket name from which to copy data. You can change it to copy from other external bucket. 'basedosdados' destination_bucket_name str Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) None mode str Optional Folder of which dataset to update. Defaults to \"staging\". 'staging' Source code in basedosdados/upload/storage.py def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name (str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Optional Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = list ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" ) ) if source_table_ref == []: raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) # Divides source_table_ref list for maximum batch request size source_table_ref_chunks = [ source_table_ref [ i : i + 999 ] for i in range ( 0 , len ( source_table_ref ), 999 ) ] for source_table in source_table_ref_chunks : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket )","title":"copy_table()"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage.delete_file","text":"Deletes file from path <bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename> . Parameters: Name Type Description Default filename str Name of the file to be deleted required mode str Folder of which dataset to update [raw|staging|all] required partitions str, pathlib.PosixPath, or dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None not_found_ok bool Optional. What to do if file not found False Source code in basedosdados/upload/storage.py def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) mode = [ \"raw\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists () or not blob . exists () and not not_found_ok : blob . delete () else : return","title":"delete_file()"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage.delete_table","text":"Deletes a table from storage, sends request in batches. Parameters: Name Type Description Default mode str Optional Folder of which dataset to update. 'staging' bucket_name str The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) None not_found_ok bool Optional. What to do if table not found False Source code in basedosdados/upload/storage.py def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. Args: mode (str): Optional Folder of which dataset to update. bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = list ( self . client [ \"storage_staging\" ] . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) ) else : table_blobs = list ( self . bucket . list_blobs ( prefix = prefix )) if table_blobs == []: if not_found_ok : return else : raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) else : # Divides table_blobs list for maximum batch request size table_blobs_chunks = [ table_blobs [ i : i + 999 ] for i in range ( 0 , len ( table_blobs ), 999 ) ] for source_table in table_blobs_chunks : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : blob . delete ()","title":"delete_table()"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage.download","text":"Download files from Google Storage from path mode / dataset_id / table_id / partitions / filename and replicate folder hierarchy on save, There are 2 modes: * raw : download file from raw mode * staging : download file from staging mode You can also use the partitions argument to choose files from a partition Parameters: Name Type Description Default filename str Optional Specify which file to download. If \" \" , downloads all files within the bucket folder. Defaults to \" \". '*' savepath str Where you want to save the data on your computer. Must be a path to a directory. '' partitions str, dict Optional If downloading a single file, use this to specify the partition path from which to download. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None mode str Optional Folder of which dataset to update.[raw/staging] 'raw' if_not_exists str Optional. What to do if data not found. 'raise' : Raises FileNotFoundError. 'pass' : Do nothing and exit the function 'raise' Exceptions: Type Description FileNotFoundError If the given path <mode>/<dataset_id>/<table_id>/<partitions>/<filename> could not be found or there are no files to download. Source code in basedosdados/upload/storage.py def download ( self , filename = \"*\" , savepath = \"\" , partitions = None , mode = \"raw\" , if_not_exists = \"raise\" , ): \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy on save, There are 2 modes: * `raw`: download file from raw mode * `staging`: download file from staging mode You can also use the `partitions` argument to choose files from a partition Args: filename (str): Optional Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\". savepath (str): Where you want to save the data on your computer. Must be a path to a directory. partitions (str, dict): Optional If downloading a single file, use this to specify the partition path from which to download. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` mode (str): Optional Folder of which dataset to update.[raw/staging] if_not_exists (str): Optional. What to do if data not found. * 'raise' : Raises FileNotFoundError. * 'pass' : Do nothing and exit the function Raises: FileNotFoundError: If the given path `<mode>/<dataset_id>/<table_id>/<partitions>/<filename>` could not be found or there are no files to download. \"\"\" # Prefix to locate files within the bucket prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # Add specific partition to search prefix if partitions : prefix += self . _resolve_partitions ( partitions ) # if no filename is passed, list all blobs within a given table if filename != \"*\" : prefix += filename blob_list = list ( self . bucket . list_blobs ( prefix = prefix )) # if there are no blobs matching the search raise FileNotFoundError or return if blob_list == []: if if_not_exists == \"raise\" : raise FileNotFoundError ( f \"Could not locate files at { prefix } \" ) else : return # download all blobs matching the search to given savepath for blob in blob_list : # parse blob.name and get the csv file name csv_name = blob . name . split ( \"/\" )[ - 1 ] # build folder path replicating storage hierarchy blob_folder = blob . name . replace ( csv_name , \"\" ) # replicate folder hierarchy ( Path ( savepath ) / blob_folder ) . mkdir ( parents = True , exist_ok = True ) # download blob to savepath blob . download_to_filename ( filename = f \" { savepath } / { blob . name } \" )","title":"download()"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage.init","text":"Initializes bucket and folders. Folder should be: raw : that contains really raw data staging : preprocessed data ready to upload to BigQuery Parameters: Name Type Description Default replace bool Optional. Whether to replace if bucket already exists False very_sure bool Optional. Are you aware that everything is going to be erased if you replace the bucket? False Exceptions: Type Description Warning very_sure argument is still False. Source code in basedosdados/upload/storage.py def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) else : self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" )","title":"init()"},{"location":"reference_api_py/#basedosdados.upload.storage.Storage.upload","text":"Upload to storage at <bucket_name>/<mode>/<dataset_id>/<table_id> . You can: Add a single file setting path = <file_path> . Add a folder with multiple files setting path = <folder_path> . The folder should just contain the files and no folders. Add partitioned files setting path = <folder_path> . This folder must follow the hive partitioning scheme i.e. <table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv (ex: mytable/country=brasil/year=2020/mypart.csv ). Remember all files must follow a single schema. Otherwise, things might fail in the future. There are 3 modes: raw : should contain raw files from datasource staging : should contain pre-treated files ready to upload to BiqQuery all : if no treatment is needed, use all . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file or folder that you want to upload to storage required mode str Folder of which dataset to update [raw|staging|all] 'all' partitions str, pathlib.PosixPath, or dict Optional. If adding a single file , use this to add it to a specific partition. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str Optional. What to do if data exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' upload_args Extra arguments accepted by google.cloud.storage.blob.Blob.upload_from_file {} Source code in basedosdados/upload/storage.py def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 3 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) mode = [ \"raw\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists != \"pass\" : raise Exception ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"Set if_exists to 'replace' to overwrite data\" )","title":"upload()"},{"location":"reference_api_py/#basedosdados.upload.dataset.Dataset","text":"Manage datasets in BigQuery.","title":"Dataset"},{"location":"reference_api_py/#basedosdados.upload.dataset.Dataset.create","text":"Creates BigQuery datasets given dataset_id . It can create two datasets: <dataset_id> (mode = 'prod') <dataset_id>_staging (mode = 'staging') If mode is all, it creates both. Parameters: Name Type Description Default mode str Optional. Which dataset to create [prod|staging|all]. 'all' if_exists str Optional. What to do if dataset exists raise : Raises Conflic exception replace : Drop all tables and replace dataset update : Update dataset description pass : Do nothing 'raise' Exceptions: Type Description Warning Dataset already exists and if_exists is set to raise Source code in basedosdados/upload/dataset.py def create ( self , mode = \"all\" , if_exists = \"raise\" ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflic exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ]) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : job = m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. except Conflict : if if_exists == \"pass\" : return else : raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) # Make prod dataset public self . publicize ()","title":"create()"},{"location":"reference_api_py/#basedosdados.upload.dataset.Dataset.delete","text":"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Parameters: Name Type Description Default mode str Optional. Which dataset to delete [prod|staging|all] 'all' Source code in basedosdados/upload/dataset.py def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True )","title":"delete()"},{"location":"reference_api_py/#basedosdados.upload.dataset.Dataset.init","text":"Initialize dataset folder at metadata_path at metadata_path/<dataset_id> . The folder should contain: dataset_config.yaml README.md Parameters: Name Type Description Default replace str Optional. Whether to replace existing folder. False Exceptions: Type Description FileExistsError If dataset folder already exists and replace is False Source code in basedosdados/upload/dataset.py def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) for file in ( Path ( self . templates ) / \"dataset\" ) . glob ( \"*\" ): if file . name in [ \"dataset_config.yaml\" , \"README.md\" ]: # Load and fill template template = self . _render_template ( f \"dataset/ { file . name } \" , dict ( dataset_id = self . dataset_id ) ) # Write file ( self . dataset_folder / file . name ) . open ( \"w\" , encoding = \"utf-8\" ) . write ( template ) # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self","title":"init()"},{"location":"reference_api_py/#basedosdados.upload.dataset.Dataset.publicize","text":"Changes IAM configuration to turn BigQuery dataset public. Parameters: Name Type Description Default mode bool Which dataset to create [prod|staging|all]. 'all' Source code in basedosdados/upload/dataset.py def publicize ( self , mode = \"all\" ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ])","title":"publicize()"},{"location":"reference_api_py/#basedosdados.upload.dataset.Dataset.update","text":"Update dataset description. Toogle mode to choose which dataset to update. Parameters: Name Type Description Default mode str Optional. Which dataset to update [prod|staging|all] 'all' Source code in basedosdados/upload/dataset.py def update ( self , mode = \"all\" ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. dataset = m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ]), fields = [ \"description\" ] ) # Make an API request.","title":"update()"},{"location":"reference_api_py/#basedosdados.upload.table.Table","text":"Manage tables in Google Cloud Storage and BigQuery.","title":"Table"},{"location":"reference_api_py/#basedosdados.upload.table.Table.append","text":"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Parameters: Name Type Description Default filepath str or pathlib.PosixPath Where to find the file that you want to upload to create a table with required partitions str, pathlib.PosixPath, dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str 0ptional. What to do if data with same name exists in storage 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Source code in basedosdados/upload/table.py def append ( self , filepath , partitions = None , if_exists = \"raise\" , ** upload_args ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing \"\"\" Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = None , if_exists = if_exists , ** upload_args , ) self . create ( if_table_exists = \"replace\" , if_table_config_exists = \"pass\" , if_storage_data_exists = \"pass\" , )","title":"append()"},{"location":"reference_api_py/#basedosdados.upload.table.Table.create","text":"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at <dataset_id>_staging.<table_id> in BigQuery. It looks for data saved in Storage at <bucket_name>/staging/<dataset_id>/<table_id>/* and builds the table. It currently supports the types: Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme <key1>=<value1>/<key2>=<value2> - for instance, year=2012/country=BR . The partition is automatcally detected by searching for partitions on the table_config.yaml . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file that you want to upload to create a table with None job_config_params dict Optional. Job configuration params from bigquery None if_table_exists str Optional What to do if table exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' force_dataset bool Creates <dataset_id> folder and BigQuery Dataset if it doesn't exists. True if_table_config_exists str Optional. What to do if config files already exist 'raise': Raises FileExistError 'replace': Replace with blank template 'pass'; Do nothing 'raise' if_storage_data_exists str Optional. What to do if data already exists on your bucket: 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. 'csv' columns_config_url str google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: \"coluna\" and column description: \"descricao\" None Source code in basedosdados/upload/table.py def create ( self , path = None , job_config_params = None , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url = None , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>` - for instance, `year=2012/country=BR`. The partition is automatcally detected by searching for `partitions` on the `table_config.yaml`. Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. columns_config_url (str): google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. The sheet must contain the column name: \"coluna\" and column description: \"descricao\" \"\"\" if path is None : # Look if table data already exists at Storage data = self . client [ \"storage_staging\" ] . list_blobs ( self . bucket_name , prefix = f \"staging/ { self . dataset_id } / { self . table_id } \" ) # Raise: Cannot create table without external data if not data : raise BaseDosDadosException ( \"You must provide a path for uploading data\" ) # Add data to storage if isinstance ( path , ( str , Path , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , columns_config_url = columns_config_url , ) table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = Datatype ( self , source_format , \"staging\" , partitioned = self . _is_partitioned () ) . external_config # Lookup if table alreay exists table_ref = None try : table_ref = self . client [ \"bigquery_staging\" ] . get_table ( self . table_full_name [ \"staging\" ] ) except google . api_core . exceptions . NotFound : pass if isinstance ( table_ref , google . cloud . bigquery . table . Table ): if if_table_exists == \"pass\" : return None elif if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table )","title":"create()"},{"location":"reference_api_py/#basedosdados.upload.table.Table.delete","text":"Deletes table in BigQuery. Parameters: Name Type Description Default mode str Table of which table to delete [prod|staging|all] required Source code in basedosdados/upload/table.py def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging|all] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True )","title":"delete()"},{"location":"reference_api_py/#basedosdados.upload.table.Table.init","text":"Initialize table folder at metadata_path at metadata_path/<dataset_id>/<table_id> . The folder should contain: table_config.yaml publish.sql You can also point to a sample of the data to auto complete columns names. Parameters: Name Type Description Default data_sample_path str, pathlib.PosixPath Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. None if_folder_exists str Optional. What to do if table folder exists 'raise' : Raises FileExistsError 'replace' : Replace folder 'pass' : Do nothing 'raise' if_table_config_exists str Optional What to do if table_config.yaml and publish.sql exists 'raise' : Raises FileExistsError 'replace' : Replace files with blank template 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. 'csv' columns_config_url str google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: \"coluna\" and column description: \"descricao\" None Exceptions: Type Description FileExistsError If folder exists and replace is False. NotImplementedError If data sample is not in supported type or format. Source code in basedosdados/upload/table.py def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url = None , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing if_table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv' is supported. Defaults to 'csv'. columns_config_url (str): google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. The sheet must contain the column name: \"coluna\" and column description: \"descricao\" Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) elif if_folder_exists == \"pass\" : return self if not data_sample_path and if_table_config_exists != \"pass\" : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) partition_columns = [] if isinstance ( data_sample_path , ( str , Path , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] columns = Datatype ( self , source_format ) . header ( data_sample_path ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : # Check if config files exists before passing if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass # Raise if no sample to determine columns elif not data_sample_path : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) else : self . _make_template ( columns , partition_columns ) elif if_table_config_exists == \"raise\" : # Check if config files already exist if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) # if config files don't exist, create them else : self . _make_template ( columns , partition_columns ) else : # Raise: without a path to data sample, should not replace config files with empty template self . _make_template ( columns , partition_columns ) if columns_config_url is not None : self . update_columns ( columns_config_url ) return self","title":"init()"},{"location":"reference_api_py/#basedosdados.upload.table.Table.publish","text":"Creates BigQuery table at production dataset. Table should be located at <dataset_id>.<table_id> . It creates a view that uses the query from <metadata_path>/<dataset_id>/<table_id>/publish.sql . Make sure that all columns from the query also exists at <metadata_path>/<dataset_id>/<table_id>/table_config.sql , including the partitions. Parameters: Name Type Description Default if_exists str Optional. What to do if table exists. 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Check if all required fields are filled Source code in basedosdados/upload/table.py def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" , encoding = \"utf-8\" ) . read () ) . result () self . update ( \"prod\" )","title":"publish()"},{"location":"reference_api_py/#basedosdados.upload.table.Table.update","text":"Updates BigQuery schema and description. Parameters: Name Type Description Default mode str Optional. Table of which table to update [prod|staging|all] 'all' not_found_ok bool Optional. What to do if table is not found True Source code in basedosdados/upload/table.py def update ( self , mode = \"all\" , not_found_ok = True ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue # if m == \"staging\": table . description = self . _render_template ( Path ( \"table/table_description.txt\" ), self . table_config ) # save table description open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , encoding = \"utf-8\" , ) . write ( table . description ) if m == \"prod\" : table . schema = self . _load_schema ( m ) self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = [ \"description\" , \"schema\" ] )","title":"update()"},{"location":"reference_api_py/#basedosdados.upload.table.Table.update_columns","text":"Fills descriptions of tables automatically using a public google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: \"coluna\" and column description: \"descricao\" Parameters: Name Type Description Default columns_config_url str google sheets URL. required Source code in basedosdados/upload/table.py def update_columns ( self , columns_config_url ): \"\"\"Fills descriptions of tables automatically using a public google sheets URL. The URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. The sheet must contain the column name: \"coluna\" and column description: \"descricao\" Args: columns_config_url (str): google sheets URL. \"\"\" ruamel = ryaml . YAML () ruamel . preserve_quotes = True ruamel . indent ( mapping = 4 , sequence = 6 , offset = 4 ) table_config_yaml = ruamel . load ( ( self . table_folder / \"table_config.yaml\" ) . open () ) if ( \"edit#gid=\" not in columns_config_url or \"https://docs.google.com/spreadsheets/d/\" not in columns_config_url or not columns_config_url . split ( \"=\" )[ 1 ] . isdigit () ): raise Exception ( \"The Google sheet url not in correct format.\" \"The url must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>\" ) df = self . _sheet_to_df ( columns_config_url ) if \"coluna\" not in df . columns . tolist (): raise Exception ( \"Column 'coluna' not found in Google the google sheet. \" \"The sheet must contain the column name: 'coluna' and column description: 'descricao'\" ) elif \"descricao\" not in df . columns . tolist (): raise Exception ( \"Column 'descricao' not found in Google the google sheet. \" \"The sheet must contain the column name: 'coluna' and column description: 'descricao'\" ) columns_parameters = zip ( df [ \"coluna\" ] . tolist (), df [ \"descricao\" ] . tolist ()) for name , description in columns_parameters : for col in table_config_yaml [ \"columns\" ]: if col [ \"name\" ] == name : col [ \"description\" ] = description ruamel . dump ( table_config_yaml , stream = self . table_folder / \"table_config.yaml\" )","title":"update_columns()"},{"location":"reference_api_r/","text":"R Esta API \u00e9 composta somente de m\u00f3dulos para requisi\u00e7\u00e3o de dados , ou seja, download e/ou carregamento de dados do projeto no seu ambiente de an\u00e1lise). Para fazer gerenciamento de dados no Google Cloud, busque as fun\u00e7\u00f5es na API de linha de comando ou em Python . A documenta\u00e7\u00e3o completa encontra-se na p\u00e1gina do CRAN do projeto, e segue baixo. Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas This browser does not support PDFs. Please download the PDF to view it: Download PDF .","title":"R"},{"location":"reference_api_r/#r","text":"Esta API \u00e9 composta somente de m\u00f3dulos para requisi\u00e7\u00e3o de dados , ou seja, download e/ou carregamento de dados do projeto no seu ambiente de an\u00e1lise). Para fazer gerenciamento de dados no Google Cloud, busque as fun\u00e7\u00f5es na API de linha de comando ou em Python . A documenta\u00e7\u00e3o completa encontra-se na p\u00e1gina do CRAN do projeto, e segue baixo. Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas This browser does not support PDFs. Please download the PDF to view it: Download PDF .","title":"R"},{"location":"style_data/","text":"Manual de estilo e diretrizes de dados Nessa se\u00e7\u00e3o listamos todos os padr\u00f5es do nosso manual de estilo e diretrizes de dados que usamos na Base dos Dados. Eles nos ajudam a manter os dados e metadados que publicamos com qualidade alta. Resumo : Nomea\u00e7\u00e3o de bases e tabelas Formatos de tabelas Nomea\u00e7\u00e3o de vari\u00e1veis Ordenamento de vari\u00e1veis Tipos de vari\u00e1veis Unidades de medida Quais vari\u00e1veis manter, quais adicionar e quais remover Limpando STRINGs Formatos de valores N\u00famero de bases por pull request Dicion\u00e1rios Diret\u00f3rios Nomea\u00e7\u00e3o de bases e tabelas Bases Nomeamos bases no formato <organization_id>_<descri\u00e7\u00e3o>, onde organization_id segue o padr\u00e3o abaixo organization_id Mundial mundo_<organizacao> Federal <sigla_pais>_<organizacao> Estadual <sigla_pais>_<sigla_uf>_<organizacao> Municipal <sigla_pais>_<sigla_uf>_<cidade>_<organizacao> Os componentes dos organization_id s\u00e3o: mundo/sigla_pais/sigla_uf/cidade : Abrang\u00eancia da organiza\u00e7\u00e3o - e n\u00e3o os dados (ex: IBGE tem abrang\u00eancia br ) organiza\u00e7\u00e3o : Nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que publicou os dados orginais (ex: ibge , tse , inep ). N\u00e3o sabe como nomear a organiza\u00e7\u00e3o? Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria br-rj-detran ) Tabelas Nomear tabelas \u00e9 algo menos estruturado e, por isso, requer bom senso. Mas temos algumas regras: Se houver tabelas para diferentes entidades, incluir a entidade no come\u00e7o do nome. Exemplo: municipio_valor , uf_valor . N\u00e3o incluir a unidade temporal no nome. Exemplo: nomear municipio , e n\u00e3o municipio_ano . Deixar nomes no singular. Exemplo: escola , e n\u00e3o escolas . Nomear de microdados as tabelas mais desagregadas. Em geral essas tem dados a n\u00edvel de pessoa ou transa\u00e7\u00e3o. Exemplos de dataset_id.table_id Mundial mundo_waze.alertas Dados de alertas do Waze de diferentes cidades. Federal br_tse_eleicoes.candidatos Dados de candidatos a cargos pol\u00edticos do TSE. Federal br_ibge_pnad.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal br_ibge_pnadc.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual br_sp_see_docentes.carga_horaria Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal br_rj_riodejaneiro_cmrj_legislativo.votacoes Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ). Formatos de tabelas Tabelas devem, na medida do poss\u00edvel, estar no formato long , ao inv\u00e9s de wide . Nomea\u00e7\u00e3o de vari\u00e1veis Nomes de vari\u00e1veis devem respeitar algumas regras: Usar ao m\u00e1ximo nomes j\u00e1 presentes no reposit\u00f3rio. Exemplos: ano , mes , id_municipio , sigla_uf , idade , cargo , resultado , votos , receita , despesa , preco , etc. Respeitar padr\u00f5es das tabelas de diret\u00f3rios. Ser o mais intuitivo, claro e extenso poss\u00edvel. Ter todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por _ . N\u00e3o incluir conectores como de , da , dos , e , a , em , etc. S\u00f3 ter o prefixo id_ quando a vari\u00e1vel representar chaves prim\u00e1rias de entidades (que eventualmente teriam uma tabela de diret\u00f3rio). Exemplos que tem: id_municipio , id_uf , id_escola , id_pessoa . Exemplos que n\u00e3o tem: rede , localizacao . Lista de prefixos permitidos nome_ , data_ , numero_ , quantidade_ , proporcao_ (vari\u00e1veis de porcentagem 0-100%), taxa_ , razao_ , indice_ , indicador_ , tipo_ , sigla_ , sequencial_ . Lista de sufixos comuns _pc (per capita) Ordenamento de vari\u00e1veis A ordem de vari\u00e1veis em tabelas \u00e9 padronizada para manter uma consist\u00eancia no reposit\u00f3rio. Nossas regras s\u00e3o: Chaves prim\u00e1rias \u00e0 esquerda, em ordem descendente de abrang\u00eancia. Exemplo de ordem: ano , sigla_uf , id_municipio , id_escola , nota_ideb . Agrupar e ordenar vari\u00e1veis por import\u00e2ncia ou temas. Tipos de vari\u00e1veis N\u00f3s utilizamos algumas das op\u00e7\u00f5es de tipos do BigQuery : STRING , INT64 , FLOAT64 , DATE , TIME , GEOGRAPHY . Quando escolher: STRING : Vari\u00e1veis de texto Chaves de vari\u00e1veis categ\u00f3ricas com dicion\u00e1rio ou diret\u00f3rio INT64 : Vari\u00e1veis de n\u00fameros inteiros com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o). FLOAT64 : Vari\u00e1veis de n\u00fameros com casas decimais com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o). DATE : Vari\u00e1veis de data no formato YYYY-MM-DD . TIME : Vari\u00e1veis de tempo no formato HH:MM:SS . GEOGRAPHY : Vari\u00e1veis de geografia. Unidades de medida A regra \u00e9 manter vari\u00e1veis com suas unidades de medida originais, com a exce\u00e7\u00e3o de vari\u00e1veis financeiras onde convertermos moedas antigas para as atuais (e.g. Cruzeiro para Real). Catalogamos unidades de medida em formato padr\u00e3o na tabela de arquitetura. Exemplos: m , km/h , BRL . Vari\u00e1veis devem ter sempre unidades de medida com base 1. Ou seja, ter BRL ao inv\u00e9s de 1000 BRL , ou pessoa ao inv\u00e9s de 1000 pessoas . Essa informa\u00e7\u00e3o, como outros metadados de colunas, s\u00e3o registradas na tabela de arquitetura da tabela. Quais vari\u00e1veis manter, quais adicionar e quais remover Mantemos nossas tabelas parcialmente normalizadas , e temos regras para quais vari\u00e1veis incluirmos em produ\u00e7\u00e3o. Elas s\u00e3o: Remover vari\u00e1veis de nomes de entidades que j\u00e1 est\u00e3o em diret\u00f3rios. Exemplo: retirar municipio da tabela que j\u00e1 inclui id_municipio . Remover vari\u00e1veis servindo de parti\u00e7\u00e3o. Exemplo: remover ano e sigla_uf se a tabela \u00e9 particionada nessas duas dimens\u00f5es. Adicionar chaves prim\u00e1rias principais para cada entidade j\u00e1 existente. Exemplo: adicionar id_municipio a tabelas que s\u00f3 incluem id_municipio_tse . Manter todas as chaves prim\u00e1rias que j\u00e1 vem com a tabela, mas (1) adicionar chaves relevantes (e.g. sigla_uf , id_municipio ) e (2) retirar chaves irrelevantes (e.g. regiao ). Limpando STRINGs Vari\u00e1veis categ\u00f3ricas: inicial mai\u00fascula e resto min\u00fasculo, com acentos. STRINGs n\u00e3o-estruturadas: manter igual aos dados originais. Formatos de valores Decimal: formato americano, i.e. sempre . (ponto) ao inv\u00e9s de , (v\u00edrgula). Data: YYYY-MM-DD Hor\u00e1rio (24h): HH:MM:SS Datetime ( ISO-8601 ): YYYY-MM-DDTHH:MM:SS.sssZ Valor nulo: \"\" (csv), NULL (Python), NA (R), . ou \"\" (Stata) Propor\u00e7\u00e3o/porcentagem: entre 0-100 N\u00famero de bases por pull request Pull requests no Github devem incluir no m\u00e1ximo uma base. Ou seja, podem envolver uma ou mais tabela intra-base. Dicion\u00e1rios Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas). Para cada tabela, coluna, e cobertura temporal, cada chave mapeia unicamente um valor. Chaves n\u00e3o podem ter valores nulos. Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais. Chaves s\u00f3 podem possuir zeros \u00e0 esquerda quando o n\u00famero de d\u00edgitos da vari\u00e1vel tiver significado. Quando a vari\u00e1vel for enum padr\u00e3o, n\u00f3s excluimos os zeros \u00e0 esquerda. Exemplo: mantemos o zero \u00e0 esquerda da vari\u00e1vel br_bd_diretorios_brasil.cbo_2002:cbo_2002 , que tem seis d\u00edgitos, pois o primeiro d\u00edgito 0 significa a categoria ser do grande grupo = \"Membros das for\u00e7as armadas, policiais e bombeiros militares\" . Para outros casos, como por exemplo br_inep_censo_escolar.turma:etapa_ensino , n\u00f3s excluimos os zeros \u00e0 esquerda. Ou seja, mudamos 01 para 1 . Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc. Diret\u00f3rios Diret\u00f3rios s\u00e3o as pedras fundamentais da estrutura do nosso reposit\u00f3rio. Nossas regras para gerenciar diret\u00f3rios s\u00e3o: Diret\u00f3rios representam entidades do reposit\u00f3rio que tenham chaves prim\u00e1rias (e.g. uf , munic\u00edpio , escola ) e unidades de data-tempo (e.g. data , tempo , dia , mes , ano ). Cada tabela de diret\u00f3rio tem ao menos uma chave prim\u00e1ria com valores \u00fanicos e sem nulos. Exemplos: municipio:id_municipio , uf:sigla_uf . Nomes de vari\u00e1veis com prefixo id_ s\u00e3o reservadas para chaves prim\u00e1rias de entidades. Pensou em melhorias para os padr\u00f5es definidos? Abra um issue no nosso Github ou mande uma mensagem no Discord para conversarmos :)","title":"Manual de estilo"},{"location":"style_data/#manual-de-estilo-e-diretrizes-de-dados","text":"Nessa se\u00e7\u00e3o listamos todos os padr\u00f5es do nosso manual de estilo e diretrizes de dados que usamos na Base dos Dados. Eles nos ajudam a manter os dados e metadados que publicamos com qualidade alta. Resumo : Nomea\u00e7\u00e3o de bases e tabelas Formatos de tabelas Nomea\u00e7\u00e3o de vari\u00e1veis Ordenamento de vari\u00e1veis Tipos de vari\u00e1veis Unidades de medida Quais vari\u00e1veis manter, quais adicionar e quais remover Limpando STRINGs Formatos de valores N\u00famero de bases por pull request Dicion\u00e1rios Diret\u00f3rios","title":"Manual de estilo e diretrizes de dados"},{"location":"style_data/#nomeacao-de-bases-e-tabelas","text":"","title":"Nomea\u00e7\u00e3o de bases e tabelas"},{"location":"style_data/#bases","text":"Nomeamos bases no formato <organization_id>_<descri\u00e7\u00e3o>, onde organization_id segue o padr\u00e3o abaixo organization_id Mundial mundo_<organizacao> Federal <sigla_pais>_<organizacao> Estadual <sigla_pais>_<sigla_uf>_<organizacao> Municipal <sigla_pais>_<sigla_uf>_<cidade>_<organizacao> Os componentes dos organization_id s\u00e3o: mundo/sigla_pais/sigla_uf/cidade : Abrang\u00eancia da organiza\u00e7\u00e3o - e n\u00e3o os dados (ex: IBGE tem abrang\u00eancia br ) organiza\u00e7\u00e3o : Nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que publicou os dados orginais (ex: ibge , tse , inep ). N\u00e3o sabe como nomear a organiza\u00e7\u00e3o? Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria br-rj-detran )","title":"Bases"},{"location":"style_data/#tabelas","text":"Nomear tabelas \u00e9 algo menos estruturado e, por isso, requer bom senso. Mas temos algumas regras: Se houver tabelas para diferentes entidades, incluir a entidade no come\u00e7o do nome. Exemplo: municipio_valor , uf_valor . N\u00e3o incluir a unidade temporal no nome. Exemplo: nomear municipio , e n\u00e3o municipio_ano . Deixar nomes no singular. Exemplo: escola , e n\u00e3o escolas . Nomear de microdados as tabelas mais desagregadas. Em geral essas tem dados a n\u00edvel de pessoa ou transa\u00e7\u00e3o.","title":"Tabelas"},{"location":"style_data/#exemplos-de-dataset_idtable_id","text":"Mundial mundo_waze.alertas Dados de alertas do Waze de diferentes cidades. Federal br_tse_eleicoes.candidatos Dados de candidatos a cargos pol\u00edticos do TSE. Federal br_ibge_pnad.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal br_ibge_pnadc.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual br_sp_see_docentes.carga_horaria Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal br_rj_riodejaneiro_cmrj_legislativo.votacoes Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ).","title":"Exemplos de dataset_id.table_id"},{"location":"style_data/#formatos-de-tabelas","text":"Tabelas devem, na medida do poss\u00edvel, estar no formato long , ao inv\u00e9s de wide .","title":"Formatos de tabelas"},{"location":"style_data/#nomeacao-de-variaveis","text":"Nomes de vari\u00e1veis devem respeitar algumas regras: Usar ao m\u00e1ximo nomes j\u00e1 presentes no reposit\u00f3rio. Exemplos: ano , mes , id_municipio , sigla_uf , idade , cargo , resultado , votos , receita , despesa , preco , etc. Respeitar padr\u00f5es das tabelas de diret\u00f3rios. Ser o mais intuitivo, claro e extenso poss\u00edvel. Ter todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por _ . N\u00e3o incluir conectores como de , da , dos , e , a , em , etc. S\u00f3 ter o prefixo id_ quando a vari\u00e1vel representar chaves prim\u00e1rias de entidades (que eventualmente teriam uma tabela de diret\u00f3rio). Exemplos que tem: id_municipio , id_uf , id_escola , id_pessoa . Exemplos que n\u00e3o tem: rede , localizacao . Lista de prefixos permitidos nome_ , data_ , numero_ , quantidade_ , proporcao_ (vari\u00e1veis de porcentagem 0-100%), taxa_ , razao_ , indice_ , indicador_ , tipo_ , sigla_ , sequencial_ . Lista de sufixos comuns _pc (per capita)","title":"Nomea\u00e7\u00e3o de vari\u00e1veis"},{"location":"style_data/#ordenamento-de-variaveis","text":"A ordem de vari\u00e1veis em tabelas \u00e9 padronizada para manter uma consist\u00eancia no reposit\u00f3rio. Nossas regras s\u00e3o: Chaves prim\u00e1rias \u00e0 esquerda, em ordem descendente de abrang\u00eancia. Exemplo de ordem: ano , sigla_uf , id_municipio , id_escola , nota_ideb . Agrupar e ordenar vari\u00e1veis por import\u00e2ncia ou temas.","title":"Ordenamento de vari\u00e1veis"},{"location":"style_data/#tipos-de-variaveis","text":"N\u00f3s utilizamos algumas das op\u00e7\u00f5es de tipos do BigQuery : STRING , INT64 , FLOAT64 , DATE , TIME , GEOGRAPHY . Quando escolher: STRING : Vari\u00e1veis de texto Chaves de vari\u00e1veis categ\u00f3ricas com dicion\u00e1rio ou diret\u00f3rio INT64 : Vari\u00e1veis de n\u00fameros inteiros com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o). FLOAT64 : Vari\u00e1veis de n\u00fameros com casas decimais com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o). DATE : Vari\u00e1veis de data no formato YYYY-MM-DD . TIME : Vari\u00e1veis de tempo no formato HH:MM:SS . GEOGRAPHY : Vari\u00e1veis de geografia.","title":"Tipos de vari\u00e1veis"},{"location":"style_data/#unidades-de-medida","text":"A regra \u00e9 manter vari\u00e1veis com suas unidades de medida originais, com a exce\u00e7\u00e3o de vari\u00e1veis financeiras onde convertermos moedas antigas para as atuais (e.g. Cruzeiro para Real). Catalogamos unidades de medida em formato padr\u00e3o na tabela de arquitetura. Exemplos: m , km/h , BRL . Vari\u00e1veis devem ter sempre unidades de medida com base 1. Ou seja, ter BRL ao inv\u00e9s de 1000 BRL , ou pessoa ao inv\u00e9s de 1000 pessoas . Essa informa\u00e7\u00e3o, como outros metadados de colunas, s\u00e3o registradas na tabela de arquitetura da tabela.","title":"Unidades de medida"},{"location":"style_data/#quais-variaveis-manter-quais-adicionar-e-quais-remover","text":"Mantemos nossas tabelas parcialmente normalizadas , e temos regras para quais vari\u00e1veis incluirmos em produ\u00e7\u00e3o. Elas s\u00e3o: Remover vari\u00e1veis de nomes de entidades que j\u00e1 est\u00e3o em diret\u00f3rios. Exemplo: retirar municipio da tabela que j\u00e1 inclui id_municipio . Remover vari\u00e1veis servindo de parti\u00e7\u00e3o. Exemplo: remover ano e sigla_uf se a tabela \u00e9 particionada nessas duas dimens\u00f5es. Adicionar chaves prim\u00e1rias principais para cada entidade j\u00e1 existente. Exemplo: adicionar id_municipio a tabelas que s\u00f3 incluem id_municipio_tse . Manter todas as chaves prim\u00e1rias que j\u00e1 vem com a tabela, mas (1) adicionar chaves relevantes (e.g. sigla_uf , id_municipio ) e (2) retirar chaves irrelevantes (e.g. regiao ).","title":"Quais vari\u00e1veis manter, quais adicionar e quais remover"},{"location":"style_data/#limpando-strings","text":"Vari\u00e1veis categ\u00f3ricas: inicial mai\u00fascula e resto min\u00fasculo, com acentos. STRINGs n\u00e3o-estruturadas: manter igual aos dados originais.","title":"Limpando STRINGs"},{"location":"style_data/#formatos-de-valores","text":"Decimal: formato americano, i.e. sempre . (ponto) ao inv\u00e9s de , (v\u00edrgula). Data: YYYY-MM-DD Hor\u00e1rio (24h): HH:MM:SS Datetime ( ISO-8601 ): YYYY-MM-DDTHH:MM:SS.sssZ Valor nulo: \"\" (csv), NULL (Python), NA (R), . ou \"\" (Stata) Propor\u00e7\u00e3o/porcentagem: entre 0-100","title":"Formatos de valores"},{"location":"style_data/#numero-de-bases-por-pull-request","text":"Pull requests no Github devem incluir no m\u00e1ximo uma base. Ou seja, podem envolver uma ou mais tabela intra-base.","title":"N\u00famero de bases por pull request"},{"location":"style_data/#dicionarios","text":"Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas). Para cada tabela, coluna, e cobertura temporal, cada chave mapeia unicamente um valor. Chaves n\u00e3o podem ter valores nulos. Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais. Chaves s\u00f3 podem possuir zeros \u00e0 esquerda quando o n\u00famero de d\u00edgitos da vari\u00e1vel tiver significado. Quando a vari\u00e1vel for enum padr\u00e3o, n\u00f3s excluimos os zeros \u00e0 esquerda. Exemplo: mantemos o zero \u00e0 esquerda da vari\u00e1vel br_bd_diretorios_brasil.cbo_2002:cbo_2002 , que tem seis d\u00edgitos, pois o primeiro d\u00edgito 0 significa a categoria ser do grande grupo = \"Membros das for\u00e7as armadas, policiais e bombeiros militares\" . Para outros casos, como por exemplo br_inep_censo_escolar.turma:etapa_ensino , n\u00f3s excluimos os zeros \u00e0 esquerda. Ou seja, mudamos 01 para 1 . Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc.","title":"Dicion\u00e1rios"},{"location":"style_data/#diretorios","text":"Diret\u00f3rios s\u00e3o as pedras fundamentais da estrutura do nosso reposit\u00f3rio. Nossas regras para gerenciar diret\u00f3rios s\u00e3o: Diret\u00f3rios representam entidades do reposit\u00f3rio que tenham chaves prim\u00e1rias (e.g. uf , munic\u00edpio , escola ) e unidades de data-tempo (e.g. data , tempo , dia , mes , ano ). Cada tabela de diret\u00f3rio tem ao menos uma chave prim\u00e1ria com valores \u00fanicos e sem nulos. Exemplos: municipio:id_municipio , uf:sigla_uf . Nomes de vari\u00e1veis com prefixo id_ s\u00e3o reservadas para chaves prim\u00e1rias de entidades.","title":"Diret\u00f3rios"},{"location":"style_data/#pensou-em-melhorias-para-os-padroes-definidos","text":"Abra um issue no nosso Github ou mande uma mensagem no Discord para conversarmos :)","title":"Pensou em melhorias para os padr\u00f5es definidos?"},{"location":"support/","text":"Nos apoie Financiamento coletivo \ud83d\udcb8 A Base dos Dados j\u00e1 poupou horas da sua vida? Ou permitiu coisas antes imposs\u00edveis? Nosso trabalho \u00e9 quase todo volunt\u00e1rio, mas temos v\u00e1rios custos de infraestrutura, equipe, e outros. Nos ajude a fazer esse projeto se manter e crescer! Fa\u00e7a desenvolvedores felizes Ajude a manter nosso c\u00f3digo \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Precisamos de ajuda para manter e melhorar nossos clientes Python, R, entre outros. Acesse nossos issues ou abra um novo para come\u00e7ar a desenvolver :)","title":"In\u00edcio"},{"location":"support/#nos-apoie","text":"","title":"Nos apoie"},{"location":"support/#financiamento-coletivo","text":"A Base dos Dados j\u00e1 poupou horas da sua vida? Ou permitiu coisas antes imposs\u00edveis? Nosso trabalho \u00e9 quase todo volunt\u00e1rio, mas temos v\u00e1rios custos de infraestrutura, equipe, e outros. Nos ajude a fazer esse projeto se manter e crescer! Fa\u00e7a desenvolvedores felizes","title":"Financiamento coletivo \ud83d\udcb8"},{"location":"support/#ajude-a-manter-nosso-codigo","text":"Precisamos de ajuda para manter e melhorar nossos clientes Python, R, entre outros. Acesse nossos issues ou abra um novo para come\u00e7ar a desenvolver :)","title":"Ajude a manter nosso c\u00f3digo \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb"}]}